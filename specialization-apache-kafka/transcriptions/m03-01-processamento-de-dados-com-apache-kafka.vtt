Perfeito, gente, vamos lá Terceira aula
Do nosso treinamento de Apache Kafka
Aqui a gente fala sobre Processamento de
eventos Processamento em streaming Muito
importante A gente entendeu Em macro O
que é o Kafka Ele é o famoso discão Ele
é o cara que vai ter Os dados Vai estar
armando os dados em streaming A gente
viu ontem Como que faz A ingestão Kafka
Connect, Producer Melhores práticas,
lendo de banco de dados Diferentes
Fazendo um simples método de
transformação Ou fazer o famoso SMT
dentro de Connect Transformações simples
Agora a gente começa A fazer
transformações pesadas A gente vai fazer
A modificação e gerar Dados novos
Importante dizer Que a regra ainda
continua Eu não vou optar para nada Eu
vou simplesmente ler em streaming Em
baixa latência E vou gerar dados
enriquecidos daqui dali Porque o dado
bruto Ele não tem valor para a gente De
novo, se você pegar Client ID 0078 Quem
que é Não faz sentido para a gente Agora
se assim Esse client ID 0078 Ele fez um
total Está vinculado a 10 compras Com
uma média De 5 a 6 compras por mês Que
estava dando um revenue Para a empresa
de X Aí o dado começa a ter relevância E
a gente começa a modificar A gente
começa a mudar de ID Para saber quem que
é Começa a fazer campanhas em cima
dessas pessoas Hoje em dia Eu falo que o
processamento em streaming Ele ficou
extremamente importante Pelo fato de A
gente antigamente A gente preocupava com
grupo Eu tinha um grupo de pessoas Eu
tinha lá Eu quero criar um produto Para
Meninas Da idade de 19 anos Regional Do
norte do país Então você ia criando
grupos Agora você quer um produto
especificamente Eu quero criar um
produto Para Lilian Eu quero criar um
produto para Lilian Eu quero criar um
produto para o Arthur Eu quero criar um
produto para o eu Então eu quero criar
produtos específicos Para o meu cliente
E a grande briga do mercado Independente
de qual segmento É essa E aqui nesse
momento Eu sempre falo em qualquer tipo
de arquitetura Que eu desenvolvo Em
qualquer tipo de projeto que eu
participo Mentoria, treinamento Esse é a
parte mais sensível De qualquer pipeline
de dados Independente se é beta ou sem
senso Porque aqui eu estou criando Eu
estou na verdade Materializando a minha
regra de negócio Para dentro da
tecnologia Então antes Era amontoado de
dados que estavam chegando Aqui eu
preciso transformar esse dado Na verdade
em um dado com significado Para tomada
de decisão Seja um dado analítico ou até
mesmo Para tipos de ação A gente vai
trabalhar muito com a parte de
Arquitetura reativa Beleza?
Então vamos lá Então vamos começar
Importante dizer O que é um event stream
Vamos voltar um pouquinho Vamos de novo
Com calma Essa é uma aula que eu
acredito É muito importante Mas ela é
bem mais tranquila Em questão a gente
vai mais devagar Vocês poderiam pegar
Porque realmente não pode sair daqui sem
dúvida Essa é a parte que diferencia A
utilização do pipeline de streaming Aqui
dá valor para a empresa Os outros todos
são parte do processo Aqui é onde
realmente brilha Aqui trouxe realmente
valor Porque entregou o dado correto Da
maneira que tinha que ser Para tomada de
decisão De novo Ou para tomada de uma
ação Então beleza Um event stream Ele
acontece de forma ordenada no tempo Ou
seja, ele tem um timestamp Eu vou
executando ele na matriz do tempo Então
ele vai ter Como eu comentei Lembra dos
eventos no Kafka?
A gente falou no dia 1 Eu estou mandando
um evento Ele vai ter passado, presente
e futuro Então a gente vai ter um evento
É importante dizer que Ele sempre vai
ser uma representação Do unbounded O que
é um unbounded dataset?
Bounded Eu sei o começo e o fim Então
quando eu falo que estou trabalhando Em
um batching Ou seja, o meu dado já está
Em uma camada fria Ele já terminou o
processo dele Ele está bounded Eu sei
que ele começou a outra hora Ele
terminou a outra hora E agora eu vou
executar e processar aquele dado Não é
extremamente diferente Eu sei quando
começa Mas eu não sei quando termina
Porque ele é contínuo Ele é infinito Ele
continua Pensa numa torneira aberta E a
gente vai ver que tem muito disso Como
eu comentei Vocês começaram a ver na
parte de ingestão A partir do momento
que eu ligo Alguma parte do meu processo
Eu vou começar a fazer a ingestão Eu vou
processar, eu vou entregar Você vê que
essas partes Elas não têm agendamento
Elas não têm agendamento Elas são todas
baseadas Em reação Um evento acontece
Ele reage O pipeline reage Aquilo evento
automaticamente Por quê?
Porque ele não para Consequentemente, o
custo para streaming Ele vai ser maior
do que um custo para batch Porque quando
eu falo batch Quando eu trabalho batch
Eu posso controlar custo Que ele vai ser
agendado Vai executar naquela hora Eu
vou desligar minhas máquinas Acabou
Stream não Eu ligo e não desligo mais
Então, o streaming é unbound Ele
acontece ordenado Ele é imutável Eu
comentei isso na aula Agora eu vou
reafirmar Nós não fazemos Update e
delete em um evento Um update e um
delete Na verdade, eles são novos
eventos Que vão ser appendados Dentro do
meu streaming Então, chegou um comando
de update Por exemplo Updater um
registro no banco Esse registro novo Ele
vai ser Fazer um append Dentro do meu
streaming Como registro novo Mas ele vai
ter Continuado o antigo lá Vamos supor
que eu não usei Log compaction Não tem
log compaction aqui Eu só tenho meu
change log Então, eu vou só Inserir
novos Delete Mesma coisa Então, eu só
conheço Novos Eu só atendo dados aqui O
que, consequentemente Eu vou ter uma
quantidade De dados considerável Se eu
não tiver Uma rotina de lifecycle também
Tem que tomar um cuidado com isso Um
detalhe importante O event streaming Eu
faço Eu posso fazer O repeat Eu posso
ler um evento novamente Então, eu posso
voltar no tempo Eu venho aqui e falo
Olha, agora eu vou ler Do início Do
passado Se o evento estiver ainda Não
estiver de novo, né?
Eu não tenho log compaction Eu estou
fazendo change log aqui E eu quero ler
esse cara Então, vamos desenhar aqui
rapidinho Eu estou aqui Mas eu quero ler
Esse cara aqui O que eu vou fazer?
Eu vou simplesmente Para a minha
aplicação Olha, começa a ler Do início
Detalhe importante Kafka Como a gente
viu a leitura Como é que funciona Basado
em partições e offsets Eu leio no início
da partição No primeiro offset Ou seja
Ernest Ou latest Ou seja Eu vejo no
começo Ou eu vejo no final Eu já vi
Muitas pessoas falam Ah, mas eu preciso
De novo, né?
A ideia de indexar o caso Ah, eu vou
criar um índice Porque eu quero ler
especificamente E tudo mais É um esforço
tão grande Que na quarta manhã A gente
vai ver na quarta, né?
Na quarta, na quinta, desculpa A gente
vai ver Que é mais fácil A gente usar
tecnologias Que foram feitas Para a
superfetricidade Do que simplesmente
Tentar criar coisas novas A complexidade
é tão grande É tão complexo de fazer
isso Que não vale a pena Essa que é a
realidade Beleza?
Então, eu vejo no começo Ou no fim
Então, eu vejo no começo É assim que eu
vejo Aconteceu Estou lendo no final, né?
Estou lendo no latest Aconteceu alguma
coisa?
Opa É aquele dado Estou lendo no início
Leio tudo Na próxima interação Uma vez
de novo Leio de novo Então, cada
interação que eu tiver Eu vou ler do
antes Se estiver acertado Estou olhando
Então, tomem cuidado Com essa
configuração Mas ele é Eu posso fazer o
replay Do meu evento Então, o meu evento
Não é descartado Igual acontece na
mensageria Por exemplo Um Rabbit and
Kill É a partir do momento Que eu li o
dado Descartado E não consegui ler
novamente Com aquele mesmo subscriber
Aqui não Aqui eu posso Meu consumidor
pode ler Se o evento existir Ele vai ler
O tempo que eu quiser Do início do
offset Daquela partição Até o final
Então, gente O evento acontece De forma
ordenada De novo Com aquelas regras Que
a gente comentou ontem Mas acontece De
forma ordenada Em tempo Tem um timestamp
Daquele evento Então, o evento Ele nunca
vai acontecer Exatamente no mesmo
momento Ele tem micro Milissegundos ali
E ele também Deixa eu tirar aqui Ele
também Acontece de forma imutável Eu não
posso modificar Ele lá Para pidatar um
evento Deleitar um evento na mão Não
existe isso De novo Ou eu uso Log
Compaction Para ele descartar O deles
anterior Baseado em uma chave E ter um
processo Para isso Não é um comando Que
eu vou dar Uma criação A forma como o
topo É criado E como eu trabalho isso
Mas o evento Ele é imutável Tanto que Eu
não vou modificar o evento Eu uso Log
Compaction O que?
Chave Valor A última chave O último
valor Daquela chave Eu mantenho O resto
Eu vou flegar ele Para se deletar Então,
eu descarto Aquilo ali A gente vai ver
Algumas coisas bem legais Hoje Então,
beleza Vamos falar um pouco De Event
Stream Processing Que na verdade É uma
aula de processamento Então, a gente
sabe Que dentro do Stream Eu tenho
eventos O Stream na verdade É uma
coleção de eventos Então, considera Você
é Matheus Fica no Streaming Eu falo
Stream Stream Stream Stream é um
agrupamento De eventos Então, um batch
Um grupo de eventos Eu considero ele
Como um Stream Um evento sozinho Não é
um Stream Ele pode estar dentro De um
Stream Mas ele não vai ser Um Stream
Então, o Stream É um agrupamento E eu
vou processar Aqueles eventos ali Que
acontecem Em um determinado tempo Então,
a gente Chama isso de Event Stream
Processing Ou ESP Então, a ideia É eu De
novo Os eventos acontecem Eu vou Ler
esse Stream Que é dado Naquele Stream E
vou trabalhar Ele Vou modificar
Modificar Que eu falo na verdade Criar
novos eventos Baseados naquele Stream
Que eu estou lendo Então, vamos lá Vamos
entender Alguns Métodos aqui Alguns
processos Para processamento Em Stream A
gente tem um Que a gente chama De
processamento Que é o Stream Que é um
request -response Em que eu mando Uma
request E mando a resposta Não faço nada
Eu só mando E recebo Client -server
Normalmente A gente vai ter Muitos
bancos de dados Por exemplo Eu vou no
banco Carrego um dado E volto ele Pronto
É um bom request -response Da aplicação
Com um dispositivo De armazenamento Eu
vou ter O Stream Que eu comentei Onde eu
vou trabalhar Por exemplo Um exemplo bom
aqui Processamento de Stream Eu vou
fazer Joint Streams Então, está
acontecendo A informação E eu preciso
Dar Eu preciso Ligar esses dois caras E
trazer o dado Mais mastigado De Streams
diferentes Em baixa latência Então, eu
vou ter aqui Por exemplo Music Stream Eu
vou ter User Stream Eu processo E eu
gero Um Stream novo Que é As músicas De
usuário Por país Por exemplo Então, aqui
eu tenho Um novo Coisas diferentes
Gente, muito importante Novo E eu tenho
O clássico Batch Stream Que eu faço Uma
TL Extract Transforming Load Leio de
fontes Diferentes E trago Para um DDA
Beleza?
Mas o nosso foco aqui É o Streaming
Processing Eu quero saber Como é que eu
consigo Trabalhar Em baixa latência E de
novo Eu quero transformar O meu dado Eu
quero mudar A forma dele E aí a
brincadeira De novo, né?
Muda um pouco Pessoal, até aí Me dando
um ok Todo mundo Vamos fazer aqui Uma
conferência Vamos ver se Vamos fazer um
heartbeat aqui Todo mundo ok Me dá
joinha Aí, beleza Eu gosto de perda
Deixa eu
Pode Pode ver o nosso Ô, Matheus É No
caso do Spark Streaming Que ele trabalha
Com micro -batch No caso Ele não é muito
considerado Real -time Devido à
latência, né?
Mas Qual a diferença Entre o micro
-batching E esse batching processing aí?
Na verdade O micro -batching Tá dentro
do Streaming Processing Então Ah, tá É
Nesse caso Do Spark Eu vou explicar Mas
só adiantando A diferença dele É que ele
trabalha Em 100 milissegundos É isso O
micro -batch É 100 milissegundos Hum Aí
nesse caso Não pode ser considerado Real
-time, né?
Não, sim É considerado É considerado O
Structure Streaming O Structure
Streaming, sim Pensa assim, ó É porque o
Spark Ele tem a Antiga API De Spark
Streaming E tem a Structure Streaming
Então ele trabalha Assim Em Streaming Eu
vou explicar mais Pra ficar mais claro
Mas pensa assim Ele trabalha O Spark,
sim Ele é Ele é Tempo real Só que ele
Por exemplo Eu quero uma coisa Abaixo De
100 milissegundos Hum Aí ele não Ele não
atenderia Mas eu acho que já Eu vou
confirmar Até vocês amanhã Eu acho O Jet
Light Speed Que eu vou comentar mais
tarde Já traz Já traz essa Essa
melhoria, tá?
Eu acho que ele já Tá trabalhando Numa
base Até mais baixa Mas sim O Spark
Streaming Ele trabalha Em tempo real
Real -time Perfeito Obrigado Obrigado
Marcos Não necessariamente Que eu
comentei Ele consegue Latência até 100
milissegundos E é contínuo Então Daí é
pra trabalhar Tranquilamente Em tempo
real Com o Spark Só que de novo Quanto
mais Robusto É o nosso
processamento Ele pode chegar A 100
milissegundos É a mesma coisa Com a
tecnologia Que eu vou mostrar aqui Mas
se você colocar Uma complexidade Eu tô
lendo De fontes Que são Por exemplo
Frias São cold E aí Eu vou aumentar
Minha latência Automaticamente Se eu
precisar Ah, eu vou ler agora Eu vou O
dado chegou em streaming Eu tô fazendo
As 5 joins Eu vou lá no mon Eu faço uma
query Eu vou lá no SQL E valido Aí Não
adianta Seu latência vai só Crescer Ou
então Eu faço esse streaming Vou no Data
Lake E vou pegar o histórico Dos últimos
6 meses Um mês Um mês Uma semana até
Dependendo da empresa É muita coisa
Então Volumetria Versus latência O que
fala Que é real time Ou near real time
Na verdade É o seu processo Como um todo
Não é simplesmente A tecnologia vai te
atender O Strut to Stream Por exemplo
Ele é real time Porque ele é 100
milissegundos O cápita é 10
milissegundos Beleza É uma taxa mais
baixa Mas eu ainda tô na caixa De
milissegundos Eu não tô na caixa de
segundos E é uma discussão legal A gente
vai discutir mais Sobre isso Eu quero
entrar Mais em detalhe Isso é legal Pode
falar, Marcos Ô, Matheus Em cima dessa
questão aí Do Stream Tem uma diferença
Tipo assim Das linguagens de programação
Pra lidar com isso também Quando a gente
faz A leitura No tópico Configurado pro
Stream?
Boa Excelente pergunta Depende da
tecnologia Que você vai usar Pra ler
Hoje, por exemplo No Spark Não tem
diferença mais Antigamente falavam Que
Scala Era a linguagem principal Pra
Streaming Não tem isso mais Tá?
Depende de como é Qual que vai ser A
tecnologia Que você vai usar Eu vou usar
Strut to Streaming Que é o Spark Eu vou
usar Spark Eu vou usar Clink Eu vou usar
Sansa Eu vou usar Byte to X Eu vou
mostrar Então depende, tá?
Pode falar, João Eu vou usar A ver com
essa pergunta Também eu ia perguntar
Sobre as SMTs Em que Como que elas são
Codificadas Se isso tem uma linguagem
Específica Ou se Dependendo da
tecnologia Cada uma Você vai executar Um
tipo de código Diferente ali Nas
mensagens Na hora de Processar elas Que
nem, por exemplo Você tá recebendo Um
milhão de mensagens E tá rodando Um
código Python Ao invés de, por exemplo
Um código Em Java Scala Ou Scala C, por
exemplo Quando você fala Conect É Na
hora de receber Mensagens No Dos SMTs
Java O Atual Indicado Que ele só aceita
Você desenvolver SMTs Em cima de topo de
Java Entendi Você não consegue Modificar
Porém De novo Existem casos E diversos
casos Que você consegue Trabalhar
perfeitamente Com um milhão Trinta
milhões De registros Processando em
Python O que vai depender Não é a
linguagem E sim Qual vai ser O engenho
de processamento Que você vai usar No
topo disso Por exemplo Se você tá usando
Flink Você pode fazer em SQL Pode fazer
em Python Eu vou mostrar aqui, por
exemplo Em Python Com SQL junto Você
pode fazer Milhões De registros
Tranquilamente De processamento Em
baixas maratons Porque o Flink Ele tem
uma surface Por debaixo Pra eu explicar
Como é que funciona Em Python Que te
permite isso Então a linguagem Hoje Eu
vou saber se é Com vocês É meio que A
gosto Se a empresa tiver Desenvolvedores
Java Beleza Usa Java Vou dar um exemplo
Prático Eu tô com um cliente Que ele tá
Praticamente Ele é todo Java Nós estamos
Num processo Justamente De processar
Trinta e cinco milhões De registros Por
equipamento Que ele tem De IoT Por dia
Eles estão com a previsão De
quatrocentos equipamentos Total Dito
isso Eu falo assim Caralho Qual é a
linguagem Que eu vou usar Como a gente
decidiu Usar Flink Pra processamento
Eles vão fazer em escala Java Porque
eles estão Confortáveis em escala Mas
eles estão discutindo Fazer processos em
SQL Então Nada impede É só mais a
questão De qual confortável Você tá com
aquela linguagem Hoje Hoje tá Eu falo
muito isso hoje Entendi Beleza Perfeito
Obrigado Nada Mas especificamente O SMT
Não tem como fugir Porque o SMT É Kafka
É Java tá Você pode desenvolver Qualquer
coisa Ele cria um conector Em torno De
saber Java Quero criar um SMT Java Então
é por causa Do Kafka Mas eu acho Que
isso vai mudar Eu acredito Que vai mudar
Daqui a um tempo Eu tô vendo Essa
mudança aí Talvez nem Python Talvez até
Rust Eu vou começar A apostar no Rust Eu
vou começar A estudar nele Mais a fundo
A gente falando Desse cara Eu não pus a
mão ainda Porque eu não tive tempo Pra
variar Então vamos lá Vamos Aqui já faz
o batch Vamos pra alguns Conceitos aqui
Principalmente Conceitos de
processamento De streaming Primeira
coisa Que eu trabalho Eu vou ter Três
momentos Processamento de streaming Eu
vou ter O tempo do evento Quando o
evento Foi criado Evento Na origem Event
time O evento Foi criado Eu vou ter O
momento Que ele foi Atendado Dentro do
Kafka Que é o log time Atendado Event
time Eu vou ter O processing time Então
sempre que vocês Trabalharem com
processamento Streaming Não esqueçam De
fazer um processo Ou uma função Pra
desenvolver E criar Esse é o momento Que
você dá Pra processar Porque isso Pode
ajudar muito Vocês principalmente Na
questão de Reprocessamento Back feeling
Olha Eu tenho aqui O tempo Que eu Que
aqui é bem importante A questão de tempo
Eu tenho o tempo Que eu gerei aqui Então
eu posso colocar Um timestamp aqui
Beleza Põe o timestamp E vai pra cá No
momento que ele tá Aqui dentro Ele
entrou Ele vai ter O timestamp Do Kafka
E aqui Eu vou ter O timestamp Do
processamento Que eu fiz Ou seja Liguei
meu flink Por exemplo Porque pra
processamento O flink É o cara Pra isso
aqui Liguei meu flink Tô lendo o evento
O momento que eu comecei A processar
aquele evento Eu já coloco Um processing
time Pra ele Então por boa prática A
gente coloca Lá o processing time Beleza
Sempre vamos Chapar ele lá Além disso
Temos que preocupar Com o estado Só que
isso aqui É bem interessante Quando eu
falo estado Tá vendo É o estado da
operação Então eu posso Rodar esse
estado Da operação Isso é muito
importante Muitas aplicações Muito que a
gente aprendeu É interessante Talvez Eu
não fazer local Eu ter isso externo Só
que a gente tá Trabalhando em baixo
latência Em baixo volume Então
teoricamente Com mais perta Que é na
aplicação Melhor Então Alguém já ouviu
falar De RocksDB?
Fala eu Aí pra mim Só pra ver Só pra ter
uma ideia Ou não Vou saber também Quem
nunca ouviu falar De RocksDB?
Top Top Legal Legal Legal Vocês vão ver
bastante Quando vocês começarem comigo
Tá?
Por quê?
RocksDB é um banco Em memória Quem não é
um banco Em memória?
Tecnologias como Flink Antigo Faust
Batchworks É E o próprio Card Springs
Ele precisa de Algum lugar Pra armazenar
As operações Que estão acontecendo O
estado da operação Então eu li Mudei
Fodei uma agregação Esse processo Todo
Ele tem que Guardar aquele estado Porque
ele acontece De forma De baixa latência
E se acontecer Alguma coisa Eu tenho que
ter um Feio -seio Eu tenho que conseguir
Reproduzir aquele list Em caso de falha
No meio do caminho Então é um mecanismo
Que tem Pra guardar estado Que a gente
trabalha Quando a gente processa Dados
em streaming Então se eu ouvir muito
Isso Consegue começar a estudar Mais
sobre processamento De streaming Então
Normalmente Tecnologias de streaming
Elas usam O RocksDB Como A engine De
armazenamento De estado Então o RocksDB
Pra quem não conhece Esse cara aqui Em
que já está Embedado Isso é importante
Já está Embedado Na sua Engine de
processamento Então é ideal Você Sempre
dá uma olhada Ah Quer processar Com uma
biblioteca XPTO Ela já tem RocksDB
Embedado Porque isso É garantia De
performance Beleza Eu ainda não conheço
Nenhum banco Em memória Que supere
RocksDB Pra parte de streaming De novo
Flink usa Bytefax usa Faust usava Usa
Não tem atualização Vou falar um
pouquinho Dele rapidamente Até cheio Da
apresentação Porque ele realmente Meio
que É uma biblioteca Que morreu Mas É É
Grande parte Das tecnologias De
streaming Elas usam RocksDB Para guardar
O estado E aí Sempre tem um esperto
Achei que Falava assim Ah Você vai
guardar O estado Vamos fazer externo Vou
fazer um Cassandra Gente Eu não conheço
Nenhum caso Prático E eu discuti Isso
demais Nos últimos meses Com clientes
Cara Eu conversei Com o pessoal Do Kafka
Summit Pra perguntar Gente Vocês
conhecem Algum caso Em que alguém Usa O
Vamos Dar um Agregamento De estado De um
processamento Em streaming Fora Sem ser
local Fora da engine Simplesmente Coloca
lá O Cassandra Seja qual for Externo Que
ela não conhece Então Se eu te tenho Um
caso Bem Bem Bem específico De novo Eu
não conheço Nenhum Mas eu posso Eu posso
configurar Pra guardar o estado Em vez
de ele guardar localmente Ele guardar
externo Existe essa possibilidade Mas de
novo Não é recomendável Não é melhor
prático E eu não conheço O caso disso É
só se Testamos tudo Não funciona E é
melhor Depois de uns testes Exaustivos E
a forma Como a gente está processando O
único jeito é isso Mas normalmente A
gente não vai ver isso não Beleza?
Então O estado Outra coisa Que vocês vão
ver bastante Tá?
Stream Table Duality Dentro do
Processamento Stream Eu também tenho Um
conceito de table Então eu tenho Um
conceito de table E eu tenho Um conceito
De streaming Quando eu vou criar Um
objeto Pra ser processado O streaming
Como eu comentei Lá em cima lá É um
changelog Que eu vou apendando Dados E
pensa no table Como sendo Log compaction
Ele é simplesmente O que?
Ele é Os meus Os dados Que foram
processados Do tipo table Eles vão ter
somente O último registro Baseado
naquela informação Tá?
Então o snapshot O table É o snapshot
Daquilo Então Só que o legal é Um
streaming Pode virar table E um table
Pode virar streaming Tá confuso agora
Mas eu vou explicar isso Na sexta -feira
Tá?
Sexta -feira vai ter Prática disso Vou
mostrar Um processo Em streaming Tá?
Streaming Aqui Do objeto stream Vira um
table E um table Pra stream Então a
gente pode fazer isso Pelo fato de ter A
dualidade Beleza?
Quem não entendeu Fica tranquilo Que eu
vou entrar mais Em detalhe Na sexta Mas
só entendem Que Eu tenho Esses dois
tipos De output Quando eu vou processar
O meu dado Em tempo real Eu posso gerar
um stream Eu posso gerar um table A
gente faz as tecnologias Que são
voltadas Pra streamer E elas trabalham
Dessa forma Matheus Eu tenho um Spark Eu
tô trabalhando Com data frame E eu tô
lendo Struture stream E eu tipo Day
Normalmente Ele trabalha com streaming
Tá?
E a gente vai ver Que nas literaturas De
streaming Vai explicar Quando eu quero
mudar De objeto De um pro outro Beleza?
Então aqui é O streaming do wallet Gente
Pegaram
aqui Querem que eu Volte os conceitos
Que a gente vai entrar Agora no Kafka
Sprint Agora eu vou explicar O que é o
Kafka Sprint Na real Eu vou Vamos
começar A fazer o drill down dele E eu
fiz até uma prática Vocês conseguiram
Algo inédito Eu codifiquei Java Hoje
Difícil de funcionar Eu apanho O WOM
Condenado Nesse negócio Mas eu queria
Mostrar pra vocês Aqui Então tem uma
aplicação Em Kafka Streams Aqui Em Java
Porque no Kafka Stream Da Scala Eu já
Logico A GBT me ajudou Mas mesmo assim
Mesmo com a GBT Eu sempre apanhei Pra
Scala Eu sempre apanhei Não sou javeiro
Respeito muito Quem é Mas Eu trouxe pra
vocês Aqui Pra vocês poderem ver Beleza?
Então vamos lá Por enquanto Dúvidas Todo
mundo Joinha Eu vou comprar Pelos
joinhas Na tela Viu?
Joinha Joinha Joinha Perfeito De vez em
quando Eu volto Tá?
De novo Aqui é conceito Perfeito Eu
explico Claro Claro É Estado Tá?
Quando eu faço Uma operação De
processamento Em streaming Eu tenho que
guardar O estado Daquela operação Pensa
que a metadado Tá acontecendo Eu posso
Fazer ele local Usando um Banco de
memória Que é o RocksDB Que é um banco
de memória Principalmente pra isso Tá?
Ele é Leve Ele roda em nível De
aplicação Você não tem que fazer nada
Simplesmente vai fazer Toda a
automatização Pra você lá dentro As
engenhas de processamento Já tem isso
pronto Tá?
Então O normal É eu fazer isso Local Por
quê?
Não é baixa latência Tráfico de rede
Aumenta a minha latência Se eu Sair fora
Então Quanto mais próximo Eu estiver
Melhor E lembrando que Geralmente eu vou
processar Grandes quantidades A gente
vai ver Como é que a gente consegue
Controlar isso Tá?
Então eu posso fazer local Eu posso
fazer externo Mas tem a opção De ser
externo Ou seja Eu escolho um cação Um
grão Qualquer Outro banco No CICO Que eu
posso estar trabalhando Pra agravar esse
estado Da minha operação De novo Aqui é
o estado Do processamento Tá?
Não é o dado em si não O dado Ele vai
cuspir em outro lugar Posso cuspir onde
quiser Mas a operação Que ele está
executando Eu posso gravar externamente
Recomendação Sempre usem O tipo local
Que é o nativo Das engenhas de
processamento Que vocês vão usar E não
só isso É justamente Pra ele também Se
coordenar Ele vai pegando o estado E vai
salvando E vai se coordenando Por
checkpoints Metadados Todo o processo
Porque ele não é Um processo muito
simples Sabe?
Parece que Mas não é tão simples assim
Então Além disso É como ele também Se
coordena internamente Então ele tem Essa
dependência De guardar E trabalhar com o
estado Por isso que o Flink Ele é tão
monstruoso Em questão de streaming
Porque ele foi todo Pensado Em como
tratar Esse estado De processamento De
streaming Interno dele Então o algoritmo
dele A gente vai falar dele Daqui a
pouco Foi pensado nisso Mas é
Resumidamente É dessa forma Beleza?
Resumindo Preocupa com o interno Só pra
Casos Muitas de cases Caramba Matheus
Tentei O externo Deu certo Por favor Me
chama Tá?
Se alguém Conseguiu O externo foi melhor
Do que o interno Que pô Arrebentou Me
mostra Porque Todas as peças Que eu vi
Foi horrível E eu conheci Mais clientes
Não Não é legal não Então vamos lá Kafka
Streams Kafka Streams Vamos pra cá Kafka
Streams Tá?
Kafka Streams É uma biblioteca Feita em
Java Escala Tá?
Pra microserviços Pra trabalhar Com o
tempo real Lá Que a gente Viu Nossa
linha do tempo Quando a gente Foi na
nossa Linha do tempo Lá do Kafka Eu só
abri aqui Acho que é interessante Que
vocês poderiam ver E a gente relembrar É
extremamente Importante É Pá pá pá Acho
que tá aqui Nuno
dois Eu tenho ideia De mudar A minha
explicação
É Aqui ó Vai vai vai Aqui Olha só que
legal Se
a gente vê O Kafka foi criado Em dois
mil e onze Certo?
Só que Em dois mil e dezesseis Que eles
implementaram O Kafka Streams Por quê?
Eu não comentei Do Java Saindo do Kafka
Se a gente voltar Pra nossa realidade
Aqui Kafka Streams Eu tenho Meu gestão
Agora se vamos começar É ficar mais
claro Ingerir o dado O dado tá no Kafka
Beleza?
E eu preciso Tirar o dado E eu preciso
Fazer processar o dado Muito dele Só que
Só que eu tenho Que trabalhar com API De
novo Lembra do negócio Do estado?
Eu não tenho isso pronto Então Eu tirava
Esse cara Do Kafka Processava E não
seguia No Kafka mais Porque eu não tenho
Essa cidade mais Então o Spark Conectava
Por exemplo Que tava sendo Muito
utilizado O Spark Teve o boom De
processamento E justamente Teve o Spark
Stream Que foi a primeira API de Stream
Do Spark Então eu trouxe Esse cara E
dali em diante Eu não precisava Mais
estar dentro dele Pensando nisso O
pessoal falou
assim Não gente Não Nós já temos já API
de integração Para o Connect Nós temos
já APIs de botão De consumer Vamos criar
então Já que já tem Integração Para o
Connect Vamos criar uma nova Abstração
Produtor de consumer Para a gente poder
Processar os dados Porque todo mundo Tá
usando E tá saindo Do nosso cartão Eu
quero que Mantenha aqui dentro Eu quero
continuar Tendo a minha entrega Fim a
fim Dentro do Kafka E faz sentido Tá?
Então foi criado O Kafka Streams Que é
justamente De novo Uma biblioteca Em
parte Em escala Para processamento De
dados Então ela faz O que a producer API
E o consumer API faz Ela lê o dado Ela
transforma Ele agrega ele Da forma que
eu quiser E ela Ela consome E ela produz
Tudo na biblioteca Tá?
Então de novo Aqui ó Eu tenho API que
insere Eu Processo Aquele dado E eu
volto O dado Processado Para o Kafka
Porque o Kafka Streams Kafka Connect
Esquema Tudo que eu tô falando Com vocês
aqui Ele faz parte Da surface Ecosistema
Do Kafka Então eles são Dependentes Do
produzir o preço Então toda parte Eu
tenho que Eu não volto fora Eu tenho que
ter Um Kafka Eu plugar E eu volto O
resultado Para o Kafka Tá?
Funciona dessa forma Foi criado Para
isso Então vamos ver Um exemplozinho
Simples aqui ó Eu produzo o dado Eu
tenho por exemplo Um sistema Né?
Eu tenho aqui Uma aplicação Feita em
Java Escala de Orders, emails E fraud Ou
seja Eu vou Ler o dado Eu vou Consumir
Eu vou entender Filtrar A gente vai ver
aqui Um exemplozinho Tá?
Bem simples né?
Porque de novo Java não é minha praia A
gente vai Filtrar aqui o dado E volto
para o Kafka A partir desse momento O
dado que Esse consumidor Tá vendo Ele
não Sabe o que tem No tópico anterior
Ele tá num tópico novo Então aqui ele
vai reagir Opa Tem um dado novo Vamos
consumir Esse dado novo Tá?
E a mesma coisa Sim que aqui E olha só
Olha a nossa É Isso aqui Leopardo Não é
um leopardo Ele usa O rocksdb Tá?
Como o Meu banco em memória Já empregado
Dentro dele Então ele já tem Já Essa
implementação Para eu salvar O estado em
memória Tá?
Então vamos Vamos pra cá Esse aqui Esse
ponto aqui É bem legal E eu posso
Escalar Então eu posso Ter várias
instâncias Da minha aplicação De Kafka
Streams Cada um vai processar Aquele aí
Pra ser mais rápido E esse finalmente
Leve Tá?
Mas de novo Leve Fazer trabalho Com Java
Scala Qual que é a anatomia Desse
processamento Tá?
Eu primeiro Tenho um source Processo Tá?
Então eu insiro O dado O dado chega Eu
tenho uma DAG chamada Stream Processor
Que vai começar A fazer as agregações
Dentro dele E cada parte Desse aqui
Gente Pensa que tá salvando O estado
Viu?
Entrou Mandou processar Aqui ó Modificou
Ele vai salvando O metadado Internamente
De tudo ali E de novo Não só Toda essa
falha Mas também A coordenação Se eu
tiver Múltiplas instâncias Porque eu tô
Com a computação Distribuída Né?
Eu tenho O stream E por final Eu tenho O
que eu chamo De sync processor Que é o
resultado Final E de Synca pra algum
lugar Que é no caso Outro top Tá?
Matheus Recomendado Fazer isso tudo Em
tópico só?
Não Sempre vocês vão Trabalhar com
tópicos Diferentes O tópico de entrada É
um O tópico de processamento É outro E o
tópico De processamento É outro E o
tópico Pro cliente Ver por exemplo Pode
ser um terceiro Lembra do processo É
muito Eu gosto muito de usar Tá?
Funciona muito bem DataVault Ou Data
Lakehouse Né?
Eu tenho Camadas diferentes Pra cada
propósito Na parte da modelagem De dados
Eu preocupo com isso Aqui não muda O que
muda São algumas nomenclaturas E aqui Eu
vou processar Chunks gigantescos Né?
Eu vou na verdade Eu fazer rápido
Naquele tempo No que eu chamo De janela
Que eu vou explicar Daqui a pouco Lembra
do Table E streaming?
Então aqui tem Uma representação legal
Dentro do CacheStreams Tá?
Eu tenho Dois tipos De table Cateble E o
global Cateble O que o global Cateble
faz Tá?
O global Cateble Ele é uma Table global
Em que Os meus Se eu tiver Múltiplas
instâncias Essas múltiplas instâncias
Vão shardear O meu dado E eu vou
conseguir Fazer isso de forma Shardeado
Tá?
Ou seja Eu vou fazer isso De forma
paralelizada Normalmente Eu vou usar o
Cateble Que é O processamento Vai ser o
único Que eu vou executar Dentro da
minha aplicação De novo Não costuma ser
Multidistribuído Não porque Tem que ser
rápido Tá?
Então O table é O último registro Se eu
pego o table Aqui ó A lista Daqui É o É
o 1 Né?
Chegou um novo registro Que continua
sendo O Charlie aqui Eu vou ter aqui Eu
vou só agregando Né?
Eu vou manter o 1 Vou agregar Eu vou só
mantendo Então Eu tenho o último
registro No table O streaming Eu tenho
tudo Olha Chegou a lista de 1 aqui
Chegou o Charlie Eu tenho a informação
De tudo que aconteceu Do passado Então
Streaming Change log E o table É como se
fosse Uma tabela mesmo Tá?
Ou log compaction Que a gente tá
aprendendo aqui Então eu tenho Esses
dois tipos Depois a gente vai ver Isso
no código Processamento Beleza?
Vamos Pra demonstração Rapidinha De novo
Hoje em dia vai ser Pá Esse parâmetro
Que a Fisquimis Ocorre de uma instância
Pra um tópico Ou pode ser Várias
estampas Pro gás mesmo tópico Na verdade
João é a segunda Né?
Eu vou ter várias instâncias Pro gás
mesmo tópico Né?
Então vai ser vários consumidores Que eu
vou carregar aquele dado ali Só que
também Eu vou tá processando isso Em
várias instâncias Tá?
Geralmente É um tópico Ou Né?
Eu posso ter Pode ser mais tópico Se eu
tiver A operação de join Né?
Que eu posso ter join aqui Não vou
mostrar join Com o carro Porque streamer
Não quer né?
Aí eles me quebram Né?
Eu Gostei de fazer um Um javazinho ali
Pra mostrar Pra vocês verem Como é que é
Bontinho Mas eu vou mostrar o join Nos
outros Tá?
E aí Você pode ler vários tópicos Pra
você poder o join Numa aplicação Ou em
várias instâncias Da aplicação E ele vai
controlar isso Pra você Por isso que o
estado Da operação Ele é muito
importante Tá?
Onde você vai armadenar O state store É
muito importante Por isso eu gosto De
sempre rodar isso Em plataformas
específicas E de novo Caso de uso Né?
Eu vou fazer também Um Um cluster
gigantesco De Spark Ou de Flink Pra
processar um filtro Pra filtrar um dado
Pra fazer simplesmente Uma Verificação
Né?
Eu vou mostrar aqui Que o ByteRex Por
exemplo Ele pode te entregar Uma coisa
bem legal Em Python Simples Né?
Então de novo Depende Do caso de uso Vai
ter caso de uso Aqui pro mais complexo E
pro mais simples Tá?
Vamos ver O meu código Que eu vou
disponibilizar Pra vocês também Lá no
repositório Lá dentro Deixa eu só dar o
zoom Alguém com
alguma dúvida Por enquanto?
Dúvidas?
Ótimo Eu também Pode perguntar Enquanto
isso Eu vou mudando aqui Acho que já
coloquei Pro segredo Vai Sobre essa
questão De armazenar o estado Como é que
funciona Dentro da AWS?
A gente precisa subir Um EC2 E instalar
o RaxDB?
Não Dentro da engine De processamento
Que você vai usar Já vai ter embedado Já
a implementação Do EC2 Por exemplo Tem
um Flink O próprio Plot de Flink Já vai
ter a implementação Do Rocklead Pra
rodar internamente Não precisa preocupar
Com a implementação Você não tem
Gerência sobre esse cara Não Pode ficar
tranquilo É só porque tem gente Que quer
colocar Externo Eu quero externo Eu não
quero Que o processamento Esteja no
mesmo Lugar da engine Teria que subir Um
EC2 Subir um Cassandra Por exemplo E
configurar Sua aplicação De stream Pra
mandar O estado Pro Cassandra No caso Do
Kafka O Kafka É o MSK Teria que fazer
Uma conexão Junto com o servidor Do
Flink?
Isso Pra processar Sim O Flink vai
conectar No MSK Vai trazer o topo E
trabalhar com ele Entendi Valeu Ótimo E
se um topo Revisar 3 .1s Como corre a
garantia De transformar As mensagens Uma
vez que é gerado Um topo Perfeito 1 Sua
pergunta É muito boa Eu vou até repetir
ela E se for um topo Não um topo Tá
Pensa num processo Beleza?
Não um topo Que insiga Que dá 3 .1s
semânticas Eu tenho um processo Desde a
produção Até a ponta Eu quero Revisar 3
.1s Semânticas Eu falei que Eu leio o
dado E eu transformo O dado Agora
novamente Sim Vocês tem Que sempre Olhar
a documentação Na engine de
processamento Se ela te entrega Exato Em
uma semântica Sim a fim Spark Te entrega
Constructed stream Kafka stream Te
entrega Constructed stream O Flink Te
entrega Constructed stream Sem fazer
configuração Nenhum E já são baseados Já
em exato Em uma semântica Então Já te
entregam Isso Isso é importante Eu
estava esperando Alguém perguntar isso
Justamente Porque Sempre que vocês Forem
pensar E aí Que é o pulo do gato Sendo
que vocês Forem pensar Numa arquitetura
De streaming Não é somente No caso Tem
que pensar Tem que pensar Quem vai
produzir Quem vai consumir Quem vai
processar Quem vai Ler isso De um store
Se ele vai Te entregar Exato Em uma
semântica Sim a fim Mas sim João Você
pensa Nesse caso A engine Tem que
entregar Esse estado Seria no caso O
Spark O Flink Ele
faz Se você for usar O Spark O Flink Ele
vai te garantir Ou o Kafka Streams Nesse
caso aqui Vamos lá Pessoal Gente
Assustem
Nossa Estou feliz demais Que eu consegui
fazer isso Eu consegui parar Para fazer
Eu não ia fazer Isso Eu parei Hoje Já
que estou com Muita coisa de jantar Já
vou começar A fazer essa parte aqui
Então Meus
amigos Aqui está Minha classe Java
Javona Dentro do meu Paul Eu tenho aqui
As minhas Dependências Do Da minha
aplicação Java Então aqui Eu tenho aqui
O meu Kafka Streams Que eu tenho que ter
Eu tenho client Aqui eu estou usando A
versão 3 .7 .1 Que é a mesma versão Do
cluster Poderia usar A nova Poderia
Ainda é compatível Sempre vou olhar A
compatibilidade Mas aqui eu estou usando
Tranquilamente Eu estou usando O
Confluent Ava Serializer Porque eu vou
usar O schema restro Aqui para ler O
tópico Ava Um Ava Serializer E aqui log
Beleza Que eu quero cuspir O máximo de
log Possível Para vocês poderem ver O
log acontecendo Então vamos Estartar a
aplicação Vocês vão ver Vamos para a
nossa aplicação Que ela é um Genetic Ava
Account Filter Stream Então Que
informações São principais O nome da
aplicação Sempre coloco De aplicação De
novo Eu sempre vou Bater nessa tecla Não
se esqueçam disso A configuração Do meu
cluster O meu schema registry Que eu vou
estar trabalhando E aqui tem muitos logs
Aqui olha Olha que legal KStream Está
vendo Eu estou criando um stream Poderia
ser um KTable Aqui por exemplo Então
aqui eu estou falando Olha Vilda para
mim Cria para mim Um KStream Que eu
quero Change log Eu não quero KTable Eu
quero Change log Então aqui ele vai
Acessar o meu O meu top Que vai
virtualizar Ele no time de execução Ele
vai lá Vai ler Vai falar assim Olha esse
cara aqui Que você está lendo Ele é a
representação Desse top Esse é o KStream
É o KStream que a gente tem Então eu vou
fazer Outro para filtrar E mapear aqui
As informações E eu vou simplesmente
Filtrar para o Tem deletado Eu quero na
verdade Todos que são Tudo que a gente
De deletar Eu não quero Eu quero só Os
atuais Eu deletei uns redes Para a gente
poder Pegar Tá E aqui Eu gero um tópico
novo Chamado K Porque esse modelo É até
um tópico aqui Chamado KStream
KStream Deleted Removed Removed Vou
botar Vou botar filtrar aqui Vou botar
filtrar aqui Removed Removed Tá Que é um
tópico Um
tópico novo E aqui a gente tem Os logs
Para a gente poder executar Esse carinha
aqui De novo Gente eu não sou Muito
jovem Então Eu vou passar para vocês O
principal Para quem Por exemplo Eu fiz a
prova De certificação Do Kafka Quando eu
fiz Tinha muita pergunta também De Kafka
Streams Então o principal Que você tem
que saber O que um Kafka Streams é Como
é que ele se comporta E as diferenças
Que tem lá Dentro Eu vou depois passar
No último dia Algumas orientações Para
quem tiver interessado Para fazer a
prova Eu devo fazer A minha renovação Se
tudo der certo E eu não cair Em nenhum
projeto Nesse final de ano Mais crítico
Eu quero chegar Nesse final de ano Eu
quero renovar a minha Então eu já vou Eu
já tenho o material Já Que eu já estudo
O que eu tenho Então eu posso conversar
Um pouco mais sobre isso No dia 5 Com
vocês Então beleza Vamos rodar esse cara
aqui Para ele poder cuspir Os dados lá
Ah tal importante Eu estou lendo O dado
Do nosso Do nosso Do nosso Connect né Do
nosso Connect lá Do
Do CDC Ó que legal Calma que eu vou
parar Tá
Não Não Ô João Eu vou falar Nessa semana
Vai ser Está gravado Dá para ficar
tranquilo Não tem risco não Vai perder
Eu não sei como é Que vai ser Na semana
que vem A gente pode bater Um papo
depois Mas Então vamos lá Ó Vou pausar
Aqui E ó Verdade importante Se eu não
parar Vai
viu Vai executar Até eu parar a
aplicação Vamos parar aqui Deixa eu
parar de parar E aqui ele para E faz o O
Parada Sem dar erro Nesse Dá o Então
Vamos subir Um pouquinho Para a gente
Pegar As informações Que são
Interessantes Para a gente Aqui ó Que
massa Então aqui ó Aqui já começou A
filtrar De novo Eu mandei para cuspir
Tudo tá Fiz um debuner Aqui Então ele
começou A cuspir Todos os dados Que
foram processados Então eu posso
simplesmente Também pegar a aplicação E
jogar esse log Em algum repositório Da
ASCII Eu também Fazer quais em cima
disso Para validar Para criar monitores
Em cima disso De novo Depende da
praticidade Do seu processo A ideia É
ter vários monitores Em cima disso tá
Então aqui tem Todas as informações Aqui
da execução E aqui tem as interações
Olha Deixa eu ver Mais para baixo Aqui
Vai Aqui está bem grandinho Cadê Aqui ó
Aqui ó Olha que legal Aí aqui ó ID ponte
Restore Call down Tá vendo ID ponte Ou
seja Exato Eu vou semânticos Então Ele
sempre vai validando Ó Interações Não
processei zero Processei zero Porque eu
já processei Na Na No meu top Eu já
chego no final do offset Falei ó Zero
offset Ele vai ficar esperando Ele vai
te falando ó Não tem nada Não tem nada
Não tem nada Não tem nada Então ele vai
executando E ó Que legal ID ponte ID
ponte invoco Restore logic Tá vendo ó
Olha que legal Estado Agora é running
Tudo isso aqui Tá sendo gravado Em
rocksdb Dessa execução E eu só baixei a
biblioteca tá Eu só tenho a biblioteca
Que No momento em que eu instanciei A
aplicação Embedado nela Ele tem rocksdb
Que tem memória Onde ele salva Que ele
executa Tá O rocksdb Ele fica ativo
Durante a execução da aplicação Poderia
colocar externo De novo Poderia Não é
recomendado Tá E além disso Também É Ele
cria É assim A memória É o caso que a
gente faz Também é Esse que o DB faz Ele
cria Tópicos auxiliares Dentro do Kafka
de sistema Tá Então ele cria Tópicos
dele Pra ele gravar Também esse processo
Então você tem Esse dois Esse duplo
fator De processamento Que a gente tem
Tá Então eu tenho rocksdb Vai gravar em
memória Mas eu também tenho Alguns
tópicos No Kafka que eu crio
Principalmente quando eu faço Join Tá Eu
tenho lá meus tópicos Também de sistema
Tópico De Transiliente Se não me engano
Se chama Beleza Entenderam Pegaram aqui
O Kafka Streams Ó Mudei o dado viu Vamos
ver se eu fiz Mais alguma coisa aqui
Acho que eu só filtrei Eu fiz uma coisa
simples Aqui de novo Não é Não é Não é
muito Meio forte O Java não Deixa Eu
Vamos Vamos dar uma lida Nesse
tópico Pegar aqui Ele tá aberto Terminal
aqui Deixa
eu ver O terminal aqui Vai Aqui
Terminal Então aqui Eu vou ler aquele
tópico Tá Tá em Avro Eu voltei ele em
Avro E tá lá Por então Então se a gente
ver aqui O delete dele Só tá Estringo a
Falso Aí eu deletei Alguns registros
Aqui dentro Aqui Depois eu fazer Vou
tentar fazer O contrário Contrário aqui
E Costume
Falso No meio Vamos brincar Ele fez
Próximo Vamos se Mudar um pouco
Da aplicação Né Porque eu tô fazendo Um
processo Diferente Delete
Tem fácil Fazer ao vivo Vamos fazer ao
vivo Pra gente ver As coisas Acontecendo
E Lembrando Que Né Se Algum evento
Acontecer Automaticamente Ele vai reagir
A esse processamento E vai gerar Um
processo
Novo Então aqui Vamos ver Vou parar Aqui
Vamos pegar O Nome Do meu Tópico Eu
criei Um tópico Novo Vou na Minha
Colinha Colinha Pra em Cima Do Me
atrapalha
Não E Se Eu Não Fiz Nada De Errado Eu
Vou ter Só Os Arquivos Eletrados Aqui
ó Entrou Então Eu peguei Todos Que estão
Da letra Joguei No tópico Novo De novo
Aqui é um Processamento Simples Né Vamos
Só Vendo Como É Que É De Novo Aqui Eu Tô
Criando Um Novo Eu posso Modificar Campo
E Por Aí Vai Né A gente Vai Brincar No
Fim Que Eu Mostrar Coisas Mais Complexos
Poderem
Ver Suave Meus Amigos E Amigas Vamos
Beleza Além disso Além Do nosso
Amigo E Detalhe Importante Da Kafka
Streams Foi Criado Pelos Criadores Do
Kafka Então Foi Criado Pelo Time Da
Conflant Né E O Time Que Que Ele Foi
Pensado Só Que Aí Como Tudo Na Vida Tá
Acontecendo Né E Aí Vem Até As Perguntas
Estiveram Mais Cedo Linguagem
Programação E Mais O Que Que Se Percebe
No Mercado Hoje É A Necessidade De
Transformar O Que É Complicado Para Uma
Coisa Mais Fácil Né Então Tem Que A
Confluente Pensou Olha Eu Quero
Programação O Mercado Na Verdade Somente
Quem Tá Usando Café Hoje São Empresas
São Um Time De Dados Analíticas Então
Trabalha Com SQL Então Em Cima No Topo
Do Café Foi Criado Um Rap Ou Seja Na
Verdade O Que Eu Vou Chamar Aqui
Foi Criado Nada Mais Do Que O
Interpretador Ele Pega A
Sua Informação Aqui É Uma Aplicação Aqui
Eu Tenho Um Cluster Eu Tenho Um
Componente Separado Chamando Carta Que
Se Poder Para Trabalhar No Topo De Carta
E Processar Em SQL Tá Teve Uma Atração
Legal
Em 20 Se Não Engano Ele Foi Lançado 19
Quando Eu Comecei A Trabalhar Com Kafka
Poucos Meses Que Eles Tinham Anunciado
Então 19 E 20 21 Ok 22 Eu Comecei A Ver
Poucos Por Request 23 A Conferência Me
Anuncia Daqui A Pouco Né Faz Um Grande
Anúncio Que Na Minha Visão É Não Vamos
Mais Seguir Com Kafka E Sim
O Db Se Não Me Falha A Memória Ele Parou
Na 29 Na 29 Que Eu Vi Atualização É
Muito Pouco Tá Lembro Que É Muito Pouco
Porque Eu Digo Que Eles Vão
Eventualmente Vão Tirar Ele Do Do De O
Pessoal Se O Próprio Confluente
Cloud Que Estava Dentro Que Ele Engine
No Passado No Passado Não
Vamos Ver Qual Foi O Último Build Não
Não Não Não É Teve Um
Na Dar Eles Mudaram Algumas Coisas Do
Café
Do Confluente Depois eu vou dar uma
olhagem nas versões dele.
Mas esse é um cara que era interessante
de trabalhar.
De novo, eu tinha um treinamento focado
muito nele, mas a gente gosta sempre de
colocar o que realmente vai trazer para
vocês o valor nas contas.
Se eu começar a trazer coisas que a
gente está vendo que estão sendo
desaceleradas, igual eu tinha a pulsa A,
desacelerou também, eu perco o tempo
para mostrar para vocês, entrar em
detalhes de coisas que realmente estão
sendo utilizadas.
Então, eu tirei isso do treinamento e
coloquei mais evidência outro.
Mas, resumidamente, ele fazia
visualização, você podia criar conector
também a partir do TCP ou DB, mas não
era 100%.
A gente tinha integrado no Connect, mas
não era mais para desenvolvimento.
E processamento de dados.
Você rodava queries, executava
continuamente, e você podia fazer o pull
ou o push daqueles dados on APIs também.
Então, ele não era bem um banco, mas ele
era uma...
Ele é, né?
Ele não saiu 100 % ainda, mas ele vai
sair.
Ele era um engine para você trabalhar
com SQL no topo de casa.
Então, se a gente pegar da flexibilidade
ao uso, claro, o ConsumoNPI e o
ProduçoNPI vão ser as mais flexíveis,
porque você faz o que você quiser.
Eles são as cores, né?
Então, você multiplica o que você
quiser, só que você vai ter um trabalho
muito maior para fazer algumas coisas.
Kafka Streams é o meio ali, porque tem
programação, mas, de novo, ele te
entrega algumas bibliotecas já pré
-copiladas,
algumas coisas interessantes.
E o K5DB é o mais fácil de utilizar,
porém, ele é o mais flexível, porque já
está tudo pronto.
Então, tem queries específicas, eu não
tenho toda a surface de SQLiance, por
exemplo, mas eu tenho algumas coisas que
eu posso estar utilizando para
facilitar.
Até aí, tudo bem?
Aqui é um quadro falando em questão de
formato.
Então, eu posso fazer o K5DB, o formato
JSON, AVO, CSV, o Kafka Streams,
qualquer formato, string, batch array,
REST API, K5DB, Kafka Streams não é,
porque ele é uma aplicação, né?
Então, eu não consigo chamar
especificamente ele via REST.
Ele é uma aplicação em si, ele não é um
servidor.
E o estado das minhas queries, eu gravo,
eu tenho o estado deles no K5DB, no
Kafka Streams, eu não tenho ele no K5DB.
Então, de novo, esses caras, gente,
hoje, né?
Hoje, hoje, nesse momento, eles não são,
eu vou dizer assim, empresas legadas que
já tem, que Kafka Streams usam muito
bem, funcionam muito bem, mas hoje eu
vejo uma outra vertente crescendo muito.
Eu vou falar da minha experiência, por
exemplo, dos últimos dois Kafka Streams,
dos últimos dois Kafka Summits, que, na
verdade, eu acho que devia ter sido
chamada Flink Summit, que praticamente
só falou de Flink naquele evento.
Gente, vocês querem fazer a pausa agora,
que a gente vai virar, pra falar de
Flink, Spark, ByteWax, tudo de uma vez?
Vocês querem que a gente faça a parte
teórica e ou segue?
O que vocês querem?
Vamos fazer uma votação, né?
O que vocês querem fazer?
Contem pra mim aí, que eu decidi.
Hoje é um dia mais light, né?
De novo, light, né?
Quantidade de conteúdo.
Tá, vamos fazer uma pausa.
Então, eu vou pausar a gravação, então,
aqui eu paro a gravação, e a gente volta
em 15 minutos.
Tá?
Então, 25, 40.
Então, 25, 40.
Deixa eu pausar aqui.
Deixa eu pausar aqui.