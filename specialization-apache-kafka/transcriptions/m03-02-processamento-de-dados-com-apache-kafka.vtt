Perfeito, voltamos Parte 2 Kafka Streams
K5DB, os processos De a
gente montar o processamento A gente viu
como é que o Kafka Streams Funciona,
alguém tem alguma dúvida até o momento?
Se tiver, manda no chat Que eu vou dar
uma olhada, tá?
Vamos seguir aqui De novo Vamos falar
das Eu vou falar aqui das três
principais, tá?
Hoje no mercado De novo, a gente viu
Algumas mudanças nesse espectro Aqui,
tá?
Quando a gente fala de uma biblioteca
Python para streaming Mas esses dois
aqui Principalmente o Flink, de fato É a
plataforma de processamento de streaming
Mais utilizada, testada Validada E
usadas, que a gente pode pensar Para O
que a gente chama de SCP Complex Event
Processing Então, tem várias coisas bem
legais No Flink Eu vou tentar trazer até
mais Conteúdo de Flink Para treinamentos
E YouTube e por aí vai Acho que vale
muito a pena Spark, de novo, né?
Pra quem achava que Spark Não sonhava
bem para processamento Funciona muito
bem, Structure Stream É um dos caras aí
E eu vou trazer ByteWax Alguém já ouviu
falar de ByteWax?
Por favor, quem não conhece, fala não
pra mim, por
favor Aí sim, tô feliz Tô feliz Tô feliz
Tá?
Vamos processar streaming com Python,
né, gente?
Vou mostrar pra vocês que Não só como
funciona muito bem Como Questão de
volumetria Velocidade, por aí vai Esse
cara é muito bom Mas vamos começar com
Flink, tá?
Vamos começar com O que eu considero Um
dos melhores Ou melhor tecnologia de
processamento de streaming do mercado
Hoje Então, o Flink tem essa ideia De
Boundage and Boundage Como eu comentei
As APIs dele São a DataSet, API,
DataStream, API Que são as APIs de alto
nível De baixo nível Trabalho com Kernel
Deployment e Storage Posso trabalhar com
Kafka, S3 MongoDB IOSystem Posso subir
ele dentro de um Kubernetes Posso subir
no Google, posso subir no AVN Posso
subir no Mesos Posso subir isso na Lone
Posso subir no Yarn E trabalha com o
Dataflow de dados distribuídos APIs
Table API Data, Compensate Processing
API Jelly, API para Graph E Machine
Learning API As APIs de alto nível De
como eu comentei, DataSet e DataStream
API Então a gente vai ver bastante do
Flink Por que eu decidi modificar o
treinamento Para falar mais de Flink do
que SQLDB Porque ele te entrega Python
Ele te entrega SQL E a gente vai ver
isso na prática hoje Hoje a gente vai
fazer uma aplicação simples Uma
aplicação um pouco mais complexa Uma
aplicação avançada Vocês podem ver E no
último dia também, na parte do Windows
Eu vou usar ele para mostrar Advanced
Search Developing Windows Então
tá, vamos entender como é que o Flink
nasceu Esse é importante Por isso que eu
dei um pouco mais de ênfase para ele
Então ele começou como um programa Para
você ter uma ideia Ele começou como um
de Status Fear Foi criado por PHDs na
Universidade de Berlim Eram só doutores
Que justamente Juntaram com a ideia do
tipo Como resolver o processamento em
baixo latência Eu tenho Kafka Surgindo
em PES 2014 De 2011, que já tinha o
Kafka Eu tenho a necessidade do mercado
Em o dado chega Em baixo latência, em
streaming, evento O tempo inteiro,
rápido E eu tenho que transformar esse
dado também Então aqui, em 2016 Já
adicionaram o RocksDB Para fazer o store
do estado 450 issues Olha tudo aqui
legal Vou concentrar aqui nos issues Um
ano depois, 650 issues resolvidos Em
fevereiro de 2017 12 de dezembro de 2017
900 issues Agora que só vai crescendo
Aqui, eu tenho que transformar Aqui, 780
em 2018 360 1 .300 issues 200 pessoas
contribuindo em 2023 E aqui Mais 600
issues nesse ano Com 160 pessoas
Contribuindo com o projeto ativamente E
sofreu diversas Modificações, melhorias
O foco, principalmente Do Flink nos
últimos anos Está sendo em SQL A gente
conversou Tem um podcast para quem
estiver interessado Tem um podcast que a
gente conversou Com o Tim Walter Que é
um dos PNC members do Flink Em que ele
falou Toda a comunidade está focando em
SQL Como bloquear uma interface De
processamento de stream Usando SQL como
linguagem principal Como ele vai
executar A engine resolve Mas é usar o
SQL para tudo Ele ser dinâmico, ele ser
completo Ele poder resolver qualquer
tipo de caso De processamento de stream
Tá Como é que você vai fazer isso?
Como é que funciona a arquitetura do
Flink?
Eu crio, desenvolvo a aplicação Eu tenho
lá o meu client Que vai interagir com o
meu JobManager Então eu vou ter um
componente Chamado ActorSystem Nos dois
Você vai ver que na verdade Nos três
principais componentes da aplicação
JobManager e no TaskManager No Worker
Ele também vai trabalhar o Actor E ali
ele vai alocar Chegou uma aplicação
Manda para o meu TaskManager e fala
assim Para o meu Worker ele fala Olha,
aloca para mim um slot de processamento
Para executar esse caso Olha como é que
isso funciona Que interessante Eu faço O
mesmo que eu mostrei para vocês Do Gadeg
e do KafkaStreams Olha só Source,
entrada Faço o mapping do dado E as
agregações se tiver Ou um window no
Apply E aqui eu dou o signo do dado
Então, o dado
entrou Na aplicação de dados de Source
Mapeamento Do dado como todas as
agregações Que vão acontecer Se eu tiver
chave ou window Eu vou trabalhar em uma
camada inferior E depois eu faço a
operação De OutPost do dado E olha como
é que ele funciona No nível de slot Eu
posso distribuir as minhas operações Por
isso que o Flink é sensacional para isso
Por isso que ele é rápido Por isso que
ele é ágil Porque eu posso pegar o meu
Source Eu posso ter uma parte dele
rodando Nos slots do processamento do
Worker1 Eu posso ter uma parte No
Processing2 No Processing3 E o Sync
rodando separadamente em um Então eu
posso distribuir isso Em diversos slots
de computação Deixa eu ver se eu consigo
achar rapidinho Para vocês poderem ver o
poder do Flink Apache, Flink, Alibaba
Não sei para quem Conhece Acho que é
aqui Que mostra
Acho que é aqui Um case Vou colocar case
Aqui Alibaba use Flink Para vocês terem
uma ideia Como é que ele é Como é que
ele é button tested Download Download
Download Cadê a
quantidade Cadê a quantidade Só para a
minha chave Para eu
montar para vocês aqui Acho que no Flink
Será que esse aqui não tem Será que esse
aqui tinha
Vou mostrar para vocês aqui O caso de
uso do Alibaba Que são não me engano
Trilhões de mensagens Por dia
processadas no Flink Em classe de Flink
Trilhões Acho que não vai ter aqui
também não
Depois eu mando para vocês o artigo Que
fala sobre isso Mas o Flink hoje Depois
de usado para o Alibaba Cloud Milhares
de clusters Rodando trilhões de
processamentos Simultâneos em
milissegundos e segundos Durante o dia
todo Então button tested Esse cara aí
Chegou um ponto de De novo, como é que
eu comentei Do ksqlDB Por que que eu
comentei Do ksqlDB está saindo De cena
aos poucos A Conflict em 2023 A Conflict
em 2023 Anunciou a compra da Imerok
Imerok era empresa Que estava para
lançar O primeiro produto Gerenciado de
Apache Flink Então o que a Conflict fez
Compraram eles Trouxeram para dentro do
Conflict Cloud Hoje a Conflict já está
Em GA O O Flink clusters Flink Manager
Services Ele tem um serviço gerenciado
de Flink Dentro do Conflict Você tem o
Kafka lá dentro E agora você tem o Flink
Para processar o dado lá dentro também
Então eles estão querendo criar uma
plataforma de streaming Completa, fim a
fim Ainda está faltando uma pecinha Que
eu vou falar dela amanhã Que é a parte
de OLAP que ainda não tem Então eu tenho
a parte do Kafka, eu tenho a parte de
Armazenamento, eu tenho a parte de Ser
gerenciado dentro do Conflict Cloud Eu
tenho um processamento agora com Flink
Eu tinha um ksqlDB e agora eu tenho uma
coisinha mais robusta Que é o Flink E eu
vejo Que eles vão tirar cada vez mais O
ksql para poder trazer Mais e mais o
Flink Então a jogada que eles fizeram
Foi bem inteligente Porque o Flink já
está difundido de mercado Testado, eu
tenho uma Quantidade maior de APIs Então
eu tenho a Table API Em que eu trabalho
com A estrutura DSL Para trabalhar com
Tabela, eu tenho as Core APIs Data
Stream, Data Set API SQL API que é High
Level Eu trabalho com SQL, então SQL CLI
Dentro do Flink para trabalhar com ele
Então eu tenho esse processo Muito bem
definido Já está maduro Então foi uma
sacada bem inteligente Da Conflict em
simplesmente Falar assim, olha eu tenho
Kafka Streams Mas é JavaScript Eu tenho
o ksqlDB Só que ele é SQL E ele ainda
não é Tudo tão difundido Mas eu sei que
a gente está usando O Flink, e o Flink
só ganha tração Ganha tração, ganha
tração Então eles anunciaram isso como
eu comentei Desde a última Desde março
agora que eu estive Em Londres para o
Kafka Summit Cara, era tudo Flink Era
toda hora Conflict mais Flink Kafka mais
Flink, sessões de Flink A própria
dinâmica Dentro do evento que eles
colocaram Eles colocaram Um estilo de
pelúcia Do Flink, camisa do Flink Então
assim, a Conflict realmente De novo, a
Conflict é empresa Dos criadores do
Kafka Ela entendeu que Processamento de
streaming é esse cara aqui Então eu vou
começar com um cara que é de fato O
maior deles Só que Esse aqui Eu até
peguei o treinamento louco Peguei esse
pedacinho aqui Porque eu acho que está
bem iludido Está bem explicado Nós temos
aqui o Structure Stream Structure Stream
É a API de streaming do Spark Então
antigamente eu tinha Spark Stream E
agora eu tenho A Structure Stream São
APIs diferentes Lembrando que a Spark
Stream é antiga A Structure Stream é a
nova Beleza?
Aí eu falei para ele A gente que usa
versões antigas Eu vi muita gente
confundindo ainda Eu estou rodando um
Spark Stream Não Estou rodando um
Structure Stream A nomenclatura, o nome
correto é isso Aqui de novo Eu trabalho
com Unbounded Datasets E o mais
importante que perguntaram Eu trabalho
com Microbatch Microbatch de até 100
milissegundos Então mesmo que é
Microbatching Mesmo E eu falei isso na
primeira Na primeira parte do
treinamento Betting para o Kafka não é
ruim Lembra do Linger?
Eu trabalho com Microbatch de 100
milissegundos 10 milissegundos Então
tempo de um para o outro Alguém até
perguntou isso Mas isso seria mais ou
menos o tempo de latência Que consegue
chegar Se eu for colocar em Linger sim O
Kafka consegue até 10 milissegundos Sem
nenhum tipo de Perda de performance Eu
posso reduzir a 0 evento a evento Também
zerar o Linger E aqui eu trabalho até
100 milissegundos Então Microbatch na
verdade Não é tão ruim
assim Como se fosse semântica Então aqui
eu tenho IOS IOS É exatamente Como se
fosse semântica Beleza?
Trabalho de forma distribuída Onde os
batches que eu estou trabalhando São
distribuídos nos meus clusters E o
padrão geralmente eu faço E é muito
legal do Spark Para eu utilizar Acho que
é muito legal Para eu utilizar O
meu É simplesmente Botar um reach
.stream E eu saio do reach Para um reach
.stream É bem simples A forma como esse
cara trabalha Então ele é contínuo como
eu falei 100 milissegundos por padrão E
eu crio ali meus data sets Os meus data
frames Dentro do meu structure stream
Dentro do meu processamento em Spark
Delta também aceita Eu trabalhar em
formato de streaming Então eu posso
simplesmente Ler o dado do Kafka em
stream E gravar isso em Delta E
continuar ali o meu processo De
armazenamento de processos Gerar minha
silver e gold Usando todo o processo em
stream Se eu quiser Posso de novo
Dependendo da quantidade de coisas Que
eu vou trabalhar Eu posso ter uma perda
de latência E aí eu vou punir o real
time É uma discussão legal Porque eu
sempre pergunto Para eu fazer essa
dinâmica Possivelmente mais quando
estivermos juntos Mas eu vou adiantar
aqui New real time, real time Betting É
uma questão de perspectiva Tem empresas
que consideram por exemplo Eu trabalhar
Em real time em segundo New real time
Geralmente Como é que eu gosto Eu estou
em milissegundo Até um segundo eu
considero ele como tempo real E como é o
meu pipeline se ele for reativo Detalhe
muito importante Reativo Quando eu estou
trabalhando com Uma arquitetura baseada
em eventos Quando eu estou trabalhando
com 15 minutos Até meia hora eu
considero ainda Pode ser um new real
time Pode ainda ser Vocês sabem isso
Depende como é que o meu negócio vai ver
aquela informação Depois ali eu
considero tudo como betting Depois de 30
minutos Mas aberta a discussão Depende
muito De como é que a empresa Vai
considerar um ou outro Mas geralmente eu
gosto de trabalhar De abaixo de
milissegundos O que seria um pipeline
reativo Um pipeline reativo Você vai ver
isso aqui na prática Agora por exemplo
Um dado entra Por exemplo, a gente vai
fazer isso hoje de prática E amanhã
Começa hoje, acho que vai ficar mais
claro hoje E amanhã a gente conclui isso
Então aqui É o dado de entrada É o entry
Ou raw data A partir do momento que o
dado entrou Aqui está o Kafka, por
exemplo Eu não estou aqui do lado O meu
sistema está olhando para esse tópico
Igual o Kafka Streams que eu mostrei
Está vendo Eu estou olhando para aquele
tópico E a partir do momento Em que
chegou um evento novo Vou botar o meu
evento Ele reage Ao evento Ele chegou
Vou botar algo novo E aí ele começa o
processamento E eventualmente Eu tenho
aqui Output event Então eu estou reativo
É isso que é um pipeline reativo Reativo
é evento Que a gente
chama de Event Driven Architecture Ou
Kappa também Que a gente vai ver daqui a
pouco No último dia, mas eu explico ele
depois Você vai ver que a gente está
montando um aqui Então beleza ByteWax
que vocês não conhecem Eu acho legal
mostrar Algumas coisas que vocês também
não conhecem Justamente para vocês terem
algumas ideias O ByteWax Ele é uma
biblioteca em streaming Não é o Kafka
Streams Só que é uma biblioteca Python
Que trabalha com protocolo Kafka Ou
seja, ela também funciona Para headpanda
Para PubSub Se não me engano Também acho
que eles tem já A integração Mas é uma
biblioteca em streaming Em que eu crio
uma aplicação em Python E ela é feita
para trabalhar com dados unbounded Então
todo o processo de Armazenamento de
State Store Ela tem isso embedado Não
tem que criar nada Então eu posso
simplesmente ter minha aplicação Eu
posso trabalhar de forma distribuída Com
ela também
Escalável Sem ser acelerada Antigamente
a gente tinha Esse cara
aqui Só que infelizmente esse cara Eles
pararam de evoluir a biblioteca Ela meio
que não está tendo mais Nenhum tipo de
tração É uma biblioteca Python bem
simples de utilizar Em treinamentos
anteriores Para quem já assistiu A gente
tinha a aplicação Mostrando esse cara
aqui De novo Usava o RocksDB e todo o
processo Porém eles Se não me engano em
2022 Em diante Ou 21 em diante A gente
pode até ver o último O último Git
Praticamente Tem 4 anos Que ela não tem
nada Tem 4 anos que ela não tem nenhum
tipo de evolução Mas modificaram aqui só
Adicionaram deprecated Na verdade Tem
uma nova aqui Não tinha visto
Legal Não parece com o Luan Fizeram um
fork Quanto tempo Vamos ver Está sendo
bem atualizada Interessante isso aqui
Vou colocar na minha lista Para a gente
depois ver Então o Faust Agora a gente
está vendo aqui Mas a gente usava muito
esse cara aqui Ele era um cara que a
gente usava bastante Porém entrou em
deprecated Estou vendo aqui que ele
voltou Excelente, muito bom Só que aí
surgiu esse cara no meio do caminho E
mesmo Esse fork novo Que aconteceu do
Faust Que está sendo acelerado Esse cara
aqui é open source Mas é de uma empresa
Então a aceleração dele é muito maior
Então aqui A gente tem o ByteWax Que é
uma biblioteca em Python Para a gente
trabalhar com processamento de streaming
Extremamente simples Com toda a surface
de Operadores para janela Mapeamento E
por aí vai Tem muitas coisas legais aqui
No ByteWax E a vantagem de ser Python
Então eu posso trabalhar com a
biblioteca Posso interagir com outras
bibliotecas Python Pandas, Porters Eu
posso criar, vamos lá Aqui Ler o
streaming e gerar em Iceberg Por exemplo
Então eu posso criar várias abstrações
Posso comunicar com outras Stories Como
o próprio Clickhouse Posso começar com
BigQuery Posso conectar com outros caras
Justamente para trabalhar com o cara
Perjuntamente com eles Então a gente vai
ver esse cara hoje Beleza Então ele é
uma opção Que a gente tem para trabalhar
Com o streaming Beleza?
Então antes da gente passar aqui para o
próximo Eu vou começar a mostrar Vamos
começar a mostrar Esses caras em ação
Deixa eu parar aqui A gente começa com
ByteWax Depois Flink E aí a gente vai
ver esses caras em ação
Vamos botar o PyCharm A gente já fez
muito, já vão Já
vai legal Vamos requeimar Essa é a
melhor O longo fala isso Então aqui,
ByteWax S de Simple Eu criei dois aqui,
S de Simple E olha só Como é simples A
gente trabalhar aqui Com o nosso amigo
ByteWax Então a gente chama aqui A
biblioteca E o operador, operadores Eu
vou criar um data flow Então eu vou
instanciar um data flow Aqui e o meu
conector Com o
caso Como diria Um DBA de um certo banco
Eu trabalhei Gosto muito dele Ele falava
assim O mais fácil que isso é massificar
de
latim Minhas conexões Aqui eu estou
instanciando Meu flow Meu flow id Eu
trabalhando com ele chama -se Stock data
processing Então a gente pode fazer um
spec Para ver se eu tenho algum tipo de
erro Eu vou conectar lá e vou ver se tem
algum tipo de erro Eu vou fazer um
output aqui na tela Na verdade eu vou
Desculpa, eu vou fazer um output aqui No
meu Stock Simples e fácil Eu venho aqui
Eu tenho aqui o meu Código Vamos abrir
ele aqui Tá bom o
tamanho da tela gente Vocês vão
conseguir ver
legal Legal Beleza Beleza ótimo Então
aqui eu vou rodar minha aplicação O que
eu vou fazer Eu vou deixar aqui uma Uma
outra tela aberta Que eu vou gerar dados
para o Stock Para a gente ver esses
dados processando E alimentando a nossa
Nossa tabela de produção Nossa tabela de
processamento Então aqui A gente tem
esse cara aqui De Stocks Vamos colocar
lá Dez mil Dez mil usuários Vamos gerar
dez mil transações lá dentro Então
primeiro eu vou rodar esse cara aqui
Vamos trocar o nome Também para a gente
ver diferente Eu vou chamar
De Vamos ver Chamar de quarta -feira
process Stocks Para a gente saber que
esse aqui é o de agora Bom Se eu não fiz
nada de errado Vou botar aqui Para rodar
E eu já vou vir aqui No meu DPJSON E vou
Botar para rodar também Terminal aqui
Olha lá Ele já está carregando
Processando E gravando no meu tópico de
saída Vamos fazer um meme No próximo
aqui eu vou compartilhar A porção da
tela Porque aí vocês conseguem ver mais
um terminal aqui Para a gente poder ver
Consumindo também Dá para ver aqui Deixa
eu Acho que eu peguei Coloquei
Aqui Vou pegar o nome aqui mesmo E aqui
não está nem escalado não Está só para
executar single thread Aqui a gente pode
Subir workers nele Para ficar
Toperson Meu tópico
novo Mandar ele consumir Eu estou
mostrando sempre código aqui Para vocês
irem acostumando com Inicialmente com
código Amanhã vocês vão começar a ver Os
consumers e tal Eu vou fazer mais fácil
Está lá Só que aí Eu vou falar aqui Vou
escolher alguém Para usar o meu exemplo
A Eduarda falou comigo assim Está muito
simples Vamos melhorar esse cara aqui
Uma brincadeira aqui Então vou
parar Vou no meu terminal aqui Vou parar
meu consumer Eu vou parar Esse cara aqui
também Não quero rodar esse cara agora E
eu vou usar o complexo O que eu fiz aqui
Olha que legal Esse cara aqui Ele tem um
desserilizador da mensagem Vai pegar a
mensagem Vai desserilizar ela Ele vai
calcular o valor da transação Ele vai
filtrar o menor valor E vai preparar a
mensagem output Então eu tenho funções
aqui Olha que legal Eu criei um arquivo
de função separado Em que Ele vai chamar
essa função Olha aqui a mensagem Vou
trazer para Eu quero que a mensagem
esteja no jsonload Vou trazer no formato
json para mim Eu vou acertando Eu quero
também que você inclui partição Eu quero
que você inclui
offset Então eu comecei a Acertar Eu
falei olha Vamos fazer um cálculo Pega
para mim No dado os shares E a
quantidade que eu paguei Por ação
multiplica para mim Para ver quanto eu
tenho de valor real Ou seja a quantidade
de shares Que eu tenho na minha empresa
A quantidade que eu estou pagando Para
me falar o valor que eu tenho de money
Só que aí eu falei cara não quero só
isso Eu também quero filtrar Pelo menor
valor Eu quero tirar os menores valores
na verdade Então qualquer valor que for
Menor que 10 mil Ignora Para mim nós não
queremos não Botar marado E ele vai
preparar a mensagem Baseado naqueles
eventos Da função do Kafka source
message Kafka source message Ele vai
preparar a mensagem de sync Output topic
eu vou receber Eu vou fazer um json
dumps Fazendo um encoder do tf8 E vou
levar a chave Eu quero tudo chaveado Vou
fazer um padrãozinho top Matheus tanto
de função Difícil Vamos ver aqui Então
aqui eu chamo as minhas funções Aqui a
mesma coisa Estava lá antes Flow os meus
operators E o meu conector Onde é que eu
vou conectar Difícil Vou chamar aqui de
stock data processing O id flow O meu
input O meu step de input Vai
conectar no meu broker No meu int topic
Int topic stock Vamos começar a brincar
Pega para mim Instancia para mim dentro
do data frame O process stream Mapeia
para mim o step deserilizador Que vai
deserilizar a mensagem que eu falei
Dando ok Vai calcular para mim Pega o
process stream Que você carregou aqui E
calcula esse cara para mim Deu ok Segue
o próximo passo Pega o resultado disso
aqui Desse caso aqui E traz para mim o
filtro E aqui eu fiz uma lambda para ele
fazer Como se fosse um núcleo lá dentro
Aqui tem uma função lambda aqui dentro
Onde ele vai inserir dentro do output
topic Vou fazer o inspect nas mensagens
Top debug E vou colocar o ultimo step do
meu processo Que é o output Isso é o
step Step id Então internamente dele Ele
vai estar linkando esse processo Para
rodar isso dentro do seu state store
Então o byte rex É muito simples de
resolver Muito fácil Então vamos ver
como é que vai ser E aqui de novo Vamos
rodar esse cara aqui E aí eu vou deixar
a brincadeira mais legal Eu já vou Pegar
o topic de saída Dois terminais
aqui Pessoal Vocês estão entendendo o
que eu estou fazendo?
Isso é muito importante Que vocês
entendam
Legal Eu quero depois A opinião de vocês
O que vocês acharam do byte rex Daqui a
pouco eu vou perguntar Então eu vou
pegar meu terminal
Vou dar um zoom Nesse rapazinho Então
vamos para Aqui como é
que chama meu topico Eu vou chamar ele
de
Output Internet Process Store Topic JSON
Eu sempre gosto de colocar É uma coisa
minha Eu sempre gosto de colocar no
final do topico Qual que é o tipo dele
Porque eu já sei catalogar o que é JSON
O que é abro, o que é protobuf Mas é
isso, é coisa minha Legal Então eu vou
vir aqui no meu readme Para vocês terem
acesso A isso aqui depois E é esse cara
aqui que eu vou
ler Então Se minha gente Se eu não fiz
nada de errado Vou colocar esse Esse
cara aqui Deixa ele pronto de bala Para
a
gente poder ver E eu vou instanciar a
minha complex Vou botar ela para rodar
Quando ela começar A gente já vai
começar a ver O dado Gravado aqui Quando
você quiser Então vamos ver se isso é
verdade Na verdade são menores Menores
de D Menores de 10 .000
Aí ó Aí meu processamento Bonitinho Aqui
dentro Menores de 10 .000 Então se eu
for ver Desculpa, transaction value Aqui
ó, maiores de 10 .000 Que é coluna nova
Desculpa, estou olhando a coluna
Purchase Na verdade a coluna transaction
value é a coluna nova Então se for uma
coluna nova Eu poderia também tirar a
coluna aqui E trabalhar Que a gente vai
ver isso aqui no outro caso Então aqui
Olha como é que foi rápido Python Python
gente De novo, vou falar de novo Então
dá para a gente ouvir muita coisa em
streaming com python hoje Antigamente
Não tinha bibliotecas Só tinha falso Era
nova Se a gente pegar em 2019, 2020 Em
2021 principalmente A falso A gente não
tinha Uma biblioteca específica para
trabalhar com aplicações Em streaming,
processamento Agora a gente tem Bem
vindo a Byteweb Então a Byteweb de novo
era a biblioteca Python Para
processamento de streaming Acesso Mapeio
e processo E entrega aquele dado Simples
desse jeito Não tem segredo E a gente
pode melhorar Muita coisa, pode colocar
De novo, cada vez que eu vou interagindo
Eu coloco outros elementos de interação
no streaming Eu vou pagar isso em
latência Mas pode ser um caso
interessante Eu já vi casos, por exemplo
A gente ir no Mongo ou ir no banco de
dados Fazer um enriquecimento Em
streaming e depois
trazer Boa Natan, sim, você pode
paralelizar Você pode subir instâncias
Você pode subir ele como aplicação E
colocar instâncias para subir ele
Instanciado ou distribuído Infelizmente
Ele permite você fazer isso A gente pode
discutir Mais dele posteriormente Mas
dependendo do caso Eu gosto de Mateus,
como é que você usa o Byteweb?
Mateus, ao invés de utilizar Um segundo
tópico para a escrita Mateus, ao invés
de utilizar O segundo tópico para a
escrita Poderia usar o Byteweb Como
assim?
Eu não entendi, Marcos O Byteweb é a
biblioteca Ele que
está gerando para mim O dado do segundo
tópico Então o primeiro tópico Ele está
lendo Ele permite join Então estou lendo
o tópico Estou processando Estou
disputando no segundo tópico Mas o
Byteweb é o engine Que está processando
o dado para mim Exatamente É o engine de
processamento Ele é como se fosse o
Spark Ele mesmo O que ele usa?
Ele grava dentro dele O State Store
Poderia serializar Nesse caso aqui
Mas nesse caso eu quis fazer como o
Jason mesmo Como eu quis fazer no artigo
Eu fiz no Jason Eu poderia serializar
ele em agro Por exemplo, eu vou explicar
isso Em um cara que vai ficar muito mais
fácil Ele sofre muito com vários
tópicos?
Não, João Não sofre muito não Eu posso
depois posteriormente Gravar um vídeo
específico Fazendo join de vários
tópicos Mas eu prefiro usar o Flink
Quando tem diversos tópicos Eu prefiro
Porque aí eu posso colocar regras
Diferentes Eu vou ter realmente uma
engine mais Mais poderosa para
processamento Em que ela escala de
acordo com o que eu for usar Então eu
não preciso começar com um Flink
Parrudão Eu posso começar com um Flink
pequeno Com um cluster menorzinho E eu
vou expandindo ele quando ele precisar
Então se é um caso de uso específico Aí
vem um caso que Quando eu uso o By2X
Quando eu uso o próximo que é o Flink O
By2X eu uso Quando eu vou ter um caso
específico Eu vou trabalhar com Python
Eu quero uma coisa simples Eu quero uma
coisa mutável Eu consigo ter performance
Eu consigo modificar Criar funções
Modificar algumas coisas ali Mas se eu
precisar realmente ter uma plataforma De
processamento em streaming Cara, eu
quero ter um nível do
Spark Eu quero aquele processo em baixa
latência Eu quero o melhor Eu quero
nível enterprise Para executar esse caso
Aí eu vou para o próximo exemplo Que é o
nosso amigo Flink Flink Primeiro começa
com o Flink Que a gente tem que colocar
o Jars Começa por aí Jars Quem trabalha
com Spark Sabe que
A primeira Kriptonita de engenharia de
dados é o tal do Jars Mas, se você
passou nessa etapa aqui Fique feliz ali
diante dessa tranquilidade Flink Vamos
usar o Flink Table API Que eu comentei O
Table API Eu vou trabalhar aqui tanto
com um SQL misturado com Python em
alguns momentos Eu fiz uma miscelânea
aqui Para a gente poder ver como é que a
esfera funciona Então primeiro Vou
instanciar as minhas bibliotecas Vou
instalar o PyFlink Eu estou rodando
somente internamente Estou rodando só
local Vou trabalhar com o Flink Eu vou
explicar o Windows agora Eu vou explicar
o Windows na sexta -feira O Windows é só
terça -feira Vou falar qual Jar que eu
quero trazer Porque eu tenho que trazer
meu conector de Kafka Flink SQL
Connector Jar Para Kafka E vamos começar
uma brincadeira Simples aqui devagar Eu
vou lá no meu Stock E vou trazer meu
Topic Olha como é que é simples Eu vou
criar um Create Table Podia criar no
Stream Eu vou criar um Create Table aqui
Eu vou criar uma marca d 'água Para esse
cara, o Walter Mark Aqui eu vou falar
que meu Transaction Date Vai ter
intervalo de 5 segundos Estou criando
uma marca d 'água Dentro do meu
processamento do Flink Vou explicar isso
daqui a pouco Calma, paciência Trago as
minhas colunas Aqui eu falo o tipo de
conector Olha só, Kafka Meu Topic Meu
acesso ao Kafka Qual é o IP que eu vou
conectar Qual vai ser o DNS que eu vou
conectar no Kafka Meu Group ID Lembra do
Client ID e do Group ID?
Olha aqui O formato que está Aqui no
caso é um JSON E o meu Scan Startup
Depois que ele roda, ele não para mais
Então eu vou startar ele como Eu posso
chegar e falar assim Eu quero dali para
frente Eu quero do último para frente Eu
posso começar com Latest A partir do
momento que o novo evento chegar Ele
começa a processar Gente, até aí tudo
bem Mais, eu te respondi Não é
necessário a gente serializar Porque o
próprio comandativo JSON, o próprio
Batchweb Ele faz Só porque eu te
respondi Mas mandem as dúvidas se vocês
precisarem Beleza Trouxe para cá A minha
Stock Então a minha Stock na verdade
Materializou uma tabela Em nível de
execução Dentro do meu Flink Então agora
eu falo Olha Dentro do Stock Table Chama
para mim o Path Stock Data Que é a minha
tabela Stock Data Tá E aí vamos começar
a brincar Que simples Então eu vou pegar
essa Stock Table Eu vou filtrar ela Eu
quero só por exemplo Venda Eu quero só
Venda aqui E eu quero as colunas Share E
o Total Share Sold Aqui eu vou fazer
esse cálculo aqui Da soma do meu total
de vendas Além disso Eu quero a média de
preço Na minha tabela E aqui Eu vou
querer pegar uma janela Olha que legal
isso aqui Eu vou pegar uma janela Uma
Thumbnail Window De uma em uma hora
Então ele vai pegar De hora em hora E
vai calcular isso Essa parte aqui Esse
processo que está sendo executado aqui
Vai ser uma hora de execução Pegou uma
hora do Time Center que está executando
Depois uma hora Depois uma hora Não se
preocupem que eu vou explicar de novo
Vou explicar a parte de Windowing Window
tem outros tipos Eu vou explicar isso Na
cesta Porque é mais
delicado É uma janela de Windowing Mas
por enquanto entende -se o que Window
como o nome diz É uma janela de tempo de
execução Eu vou determinar baseado nessa
janela de Windowing Que eu tenho Como é
que eu vou processar esse dado
Processado de uma em uma hora Ele pode
ter o que se chama de Overlapping Ou
seja Um evento pode acontecer duas vezes
Ele tem que acontecer Eu posso pegar É
base do meu negócio Ou vai ter Gap
Depois processa uma hora de novo De
novo É contínuo Mas é contínuo do tipo
Eu escolho qual que vai ser a forma que
eu vou processar internamente Ele não
para Ele não para Mas eu falo janelas
que eu vou executar Isso é uma excelente
prática Em alguns casos como Join por
exemplo É obrigatório usar janela Você
vai sempre ficar pegando dado o tempo
inteiro Então tem que trabalhar no nível
de janela Então eu vou agrupar E vou
executar Nesse caso aqui Eu não estou
nem colocando Eu não estou nem colocando
para o cálculo de novo não Estou
colocando na tela Eu vou fazer um print
na tela Vamos lá Eu acho que eu tenho
muito
stock Eu coloquei novos já da última vez
que ele foi lá Então eu vou rodar aqui
Vocês podem ver Total Shares Per Sold É
o
primeiro Eu não me engano não Eu vou
confirmar Mas na Lease Evaluation Não A
lógica interna dele é diferente Então
aqui Eu já trouxe os eventos Que estão
acontecendo Na primeira etapa E aqui não
para De
novo Enquanto um evento chegar Ele parou
aqui Vou pegar 9 .3 e 1 .3 .5 Eu vou
agora inserir mais dados lá dentro Então
aqui está parado Eu vou lá e vou inserir
Estou aqui mais Olha lá Começou a
carregar Está
vendo Quem perguntou de Dados de reativo
Ele reagiu a minha inserção Então quando
eu falo Que um Pipeline é reativo É
porque eu inseri no Kafka E como ele
está olhando Para o tópico do Kafka Ele
já reagiu àquele evento que está
carregando os dados Então a gente fala
de dados de participação E isso dá até o
final A gente vai ver que todos os
elementos Que eu vou mostrar aqui A
gente vai ter reação Até porque para
medicar É feito de streaming Um evento
reagindo a um processo E ali vai
encadeando Beleza?
Tá Matheus, mas aí Legal, interessante
Deixa eu estartar
só Duas ingestões aqui Eu vou ingerir
mais coisas Aqui para a gente No MongoDB
Que eu acho que é interessante É só acho
Vou colocar 10 .000 registros No MongoDB
E lá no SQL Como ele é mais devagarinho
Eu vou colocar 5 .000 Com batchs de 500
Pode ser batchs de 700 Deixa eu ir
carregando aqui E eu vou explicar A
próxima etapa aqui do nosso processo
Beleza Matheus, você mostrou um
simples Um tópico único Vamos ver um
join
Então Vou fazer um join para vocês aqui
Só que esse é um join diferente Mesma
tabela stock Lembra da nossa tabela Do
nosso tópico de MongoDB O tópico de
MongoDB Como eu comentei E alguns
tópicos de Connect Ele vai trazer um
campo chamado payload E dentro desse
campo Vai ter vários outros campos lá
dentro Como eu trouxe exatamente Como o
conector tem um tipo de SMT Lembra que a
gente não tinha um SMT Se a gente for
relembrar aqui Nosso amigo Connect Eu
não tinha um SMT Vamos lá para a gente
poder Refrescar a
memória MongoUsers Não tem SMT Beleza
Estou lendo exatamente como o conector
Está programado para ler sem modificação
Se a gente for olhar Para o nosso tópico
Deixa eu pegar aqui Para a gente poder
ler dele
Vamos clicar aqui Como é que chama
MongoDB Vou pegar lá o nome Que está lá
no nosso Amigo S R C MongoDB users Esse
cara aqui
Vou pegar esse cara aqui E olha que
Interessante que vai
ser Eu vou consumir Desse cara para a
gente só ver A estrutura dele
rapidamente Aí eu vou voltar aqui Vou
fechar esse cara Vou fechar esse cara
Vou fechar esse cara também Tá bom Chega
Olha Schema Payload E o Payload é esse
aqui todo Tem barra Não está legal Bom
De novo Eu não faço update em Em dado, o
dado é cru Ali é o dado cru Só que eu
preciso tratar e tratar para processar
ele Então o que eu fiz aqui Se a gente
fechar Eu trouxe Do jeito que está Vai
lá para mim Conecta nesse tópico E olha
Para mim Essa função Então eu peguei uma
função agora E eu estou quebrando Dentro
do JSON Dentro do Payload E estou
criando aqui as minhas linhas Falando
olha Esse aqui é o UserId Id, FirstName,
LastName Então ele primeiro cria um JSON
Dump E traz esse JSON Qual que era a
função Pega o JSON Começa a esquematizar
ele para
mim Dentro do meu Flink Então como é que
eu faço Olha, Extract UserData Que é
essa função Executa ela Numa função
temporária Então essa função Aqui Ela
vai ser criada dentro do sistema Como
uma Function mesmo Que vai ser usada
aqui E aqui Eu vou executar ela agora
aqui dentro Para eu montar A minha
tabela de usuário Beleza?
Depois disso Eu trago a minha Stock
Porque eu não tinha trago ela aqui ainda
Olha a minha StockData Agora está a
StockTable aqui dentro desse DataFrame E
faz o Join para mim O Join, eu quero
essas colunas Eu quero Symbols que vem
da minha Stock TransactionShares e tal
Eu quero o primeiro nome, o último nome
E -mail do meu usuário Eu quero a
informação do meu usuário Porque às
vezes eu quero mandar uma campanha para
ele Eu quero saber quem que é, eu quero
tratar ele com Onde está, ou se for uma
linearidade Eu posso chamar também o
primeiro nome Então eu cantei essa
opção, de novo E executo aqui As minhas
lógicas que eu tinha antes Com o dado já
Com o Join, eu já fiz o Join Agora eu
vou fazer as minhas agregações De, eu
quero a sua venda Eu quero a média de
valor Eu quero Minhas transações
baseadas Em janela Vou pegar só O
resultado final, tá?
Aqui dentro Então eu vou executar esse
cara De novo Aqui eu estou mostrando só
A execução local, tá?
Daqui a pouco a gente vai mostrar dentro
do tópico Eu estou lendo o tópico e
estou mostrando na tela Aqui para a
gente poder ver esse cara Na prática
Então se eu não fiz nada de errado Ele
vai aparecer aqui Daqui a
pouco Aí ó Daqui eu trouxe tudo, tá?
Eu nem filtrei não Aqui ó Nome,
sobrenome, e -mail Primeiro
nome, último nome Transactions Então
aqui eu estou fazendo o Join, agora eu
estou printando o Join Então aqui eu
estou printando o
Join para mim Beleza Só que aqui a gente
está printando Está na
tela Posso explicar o payload?
É porque é o seguinte Se eu uso Por isso
que eu falo a questão da configuração do
conector Tem que tomar um cuidado Pode
ou não pode ser um problema Não quer
dizer que vai ser não Se eu trago O dado
cru Sem nenhuma modificação Olha como
está a minha configuração Do meu
conector do Mongo Conexão Nome do tópico
Bad size Não tem nenhum tipo de
transformação Eu não estou fazendo
Nenhum tipo de mudança O padrão desse
conector é Traz os dados para um campo
Chamado payload Que a gente viu aqui
Está vendo aqui E o JSON do documento do
Mongo Está aqui dentro Então para o
Kafka Não existe o esquema Id Aqui Não
existe id, data, operation type First
name Last name Não tem isso Está dentro
do JSON Que é o campo E aqui dentro Tem
um
JSON Não tem problema Contanto que Isso
para a gente Esquematizar e processar
Não funciona Porque como eu vou ler um
dado Se eu não esquematizei Eu teria que
esquematizar No processamento Eu trago
como payload Aqui O que é o dado Do
jeito que está no Kafka O Kafka está
assim Traz como string o payload Que só
existe payload como campo Dentro desse
tópico aqui E como ele está fazendo
string Desse cara Eu criei uma função
Para transformar o string em JSON E
quebrar os meus campos E colocar como se
fosse um modo
tabular Esse nome é um valor fixo
Perfeito Normalmente Quando um tópico Um
conector Ele está cru Ou dependendo da
configuração do conector Ele vai trazer
essa metáfora de payload Tem conectores
que já traz tudo pronto O JDBC por
exemplo Como ele está lendo o dado da
tabela Ele já traz o dado Com os campos
Se ele conseguir interpretar aqueles
campos Fazer uma infra esquema Mas se
ele não conseguir Eu vou trazer como
payload Aí vai ter um string Um JSON
grandão com toda a informação Só que
para a gente o processamento é inútil
Porque se eu for ler Eu não vou ler
legal Primeiro eu tenho que tratar o
dado Lembra que o processamento é a
parte de tratamento Eu estou preparando
o dado para ser utilizado Pegaram aí
todo mundo?
Todo mundo on?
Gente mais 15 minutos Eu termino Hoje
não vai chegar nas 10 Eu vou mostrar o
Flink agora E aí meus amigos E aí amigas
Eu terminei aqui Mas eu falei cara não
está legal Vamos fazer A gente fez dois
Um simples Um join com dois streams
Vamos fazer agora um join Nesse nível
aqui Que é o mundo real Eu tenho dados
de aplicação Então tem dados de stock Eu
tenho dados do meu Mongo Legal Meu Mongo
faz todo o sentido Então eu tenho uma
aplicação Outra aplicação E agora Vamos
pegar uma terceira aplicação Ou seja
Esse tópico aqui é o tópico Do meu amigo
CDC Então eu vou ler um tópico Está no
CDC em Avro Olha só que legal Avro Eu
vou ler um JSON Que está vindo do meu
Mongo E um JSON de
aplicação E aí o que eu estou fazendo
aqui Eu vou esquematizar Esses caras De
novo vou quebrar Vou selecionar as
colunas que eu quero Então eu quero tudo
Normalmente é um processamento de
streaming Eu não vou trazer tudo de tudo
Eu vou fazer aquilo que eu realmente
preciso fazer Então eu vou aqui dar uma
limpada Vou tirar algumas colunas Vou
renomear algumas User ID por exemplo Ele
está em diversas colunas com o mesmo
nome Vou tirar um bico Vai ficar um bico
Vou trazer o stock E vou fazer um join
dos
três Então eu vou fazer um join dos três
Eu vou Filtrar esse cara Eu só quero
venda que for maior que 500 Dólares para
mim O valor de venda meu tem que ser Que
o stock está assim E aqui eu vou colocar
um premium stock data Vou trocar o nome
aqui Para tirar um tópico novo Eu vou
falar assim Olha o sync que legal Sync é
Eu vou criar uma tabela de sync Conector
Kafka O tópico não existe E eu vou fazer
um execute insert Gente Não é coisa mais
fácil que
isso não Executa isso aqui Para mim Só
que
aqui Filter data O resultado disso aqui
Eu vou jogar aqui
dentro Fácil né Vamos ver se o negócio
for dando?
Eu vou chamar de User Stock User Account
Filtrados Filtrados Olha o
premium Eu gosto do premium Nós vamos
agora colocar Esse cara aqui Para a
gente poder ver ele Opa Fechar sim Vou
colocar para rodar
Tópico novo Assim que ele começar A
gente
começa a consumir Qual que é a ideia?
Eu vou ter sistemas Lendo, carregando,
inserindo Olha só gente Parece que a
gente vai fazendo aqui Eu vou ter SQL
gerindo dados Mongo ingerindo dados E
aplicação ingerindo dados Tornando essas
três caras Trazendo os dados deles e
colocando dentro Do Kafka Do Kafka Eu
estou processando dados em streaming Do
Do Desses camaradas Então se eu der um
enter aqui agora Ele não vai mostrar
aqui na tela não Eu vou ter dados Sendo
carregados lá Se não deu nada errado Ele
vai estar lá dentro Vamos esperar Só que
Matheus pode ser mentira Então
vamos fazer o seguinte Eu vou rodar de
novo As minhas aplicações De novo Eu vou
rodar as fontes Para a gente ver ele
Reverberando em tudo Então vamos lá no
Mongo Bom não é isso que ele serve Eu
acho que ainda está rodando ainda Parece
que ele está rodando ainda Parece que
ele é mais demoradinho Eu vou rodar no
Mongo mais um Churlauzinho O Mongo é
mais rápido Vou botar 10 mil
Se não eu vou forçar aqui No curso
ilimitado Então aqui Vou rodar aqui o
Mongo Minha aplicação JSON E vamos
deixar Para enfervir
aqui Olha só Eu estou inserindo Ele está
fazendo o cálculo Do Join Então se a
inserção fosse mais rápida Aqui o que
conta gente É que a minha inserção está
vindo no banco Do banco está indo para o
Kafka Connect No Kafka Eu estou lendo o
Flink E estou mostrando aqui na tela Eu
estou lendo o dotópico que o Flink está
gerando Ou seja Produzi Armazenei
Processei
Entenderam?
Gostaram?
Eu acho isso super legal Gostaram?
Acharam legal?
Foda muito Por favor interajam comigo
Então tá Então a gente já começou
a entender Que o nosso pipeline está
tomando forma Está tomando forma E olha
que está tomando forma No sentido de
tipo Eu estou usando praticamente Só
Kafka e Flink aqui agora Só Kafka e
Flink praticamente Porque fonte eu não
vou contar Pensei no Kafka Estou usando
o Flink para processar A gente vai
encerrar Com a parte agora do Design
Partner Para vocês poderem ver A gente
encerra por hoje Então mais 15 minutos
no máximo Eu vou deixar rodando aqui
Para a gente ter insumo para amanhã E
amanhã a gente fecha com chave de ouro
Porque amanhã tudo o que a gente fez
aqui Eu vou consumir Então deixa eu
Parar de compartilhar E voltar a
compartilhar Essa tela
Espero que vocês já estejam começando A
pensar em como vocês conseguem aplicar
No dia a dia de vocês Talvez esse
projeto funcionaria legal Começa a
pensar Eu vou sempre entregar para vocês
Ferramentas para vocês pensarem Onde que
vocês vão encaixar Poderia pensar assim
A gente vai conversar isso muito Quando
estivermos juntos Espero que vocês
Pensem de
novo Estamos todos aqui no mesmo time
São meus engenheiros e arquitetos
Posteriormente Então vamos lá Falei de
Batchworks De novo Uma comparação Entre
esses caras aqui Geralmente o Flink Ele
é uma API Mais robusta Eu gosto Ele tem
um agenciamento automático De memória
Então trabalha com Snapshot Exatamente
como a Semantics Fim a fim Então o que a
gente trabalha com o Flink é Leio do
Kafka Processo Volta para o Kafka o dado
processado Pronto para ser já consumido
Tipo assim Gente está pronto pode usar
Não tem que fazer mais nada Eu trabalho
com Spark Geralmente o que eu faço Spark
Trabalho no projeto Delta E eu devolvo
para as aplicações de downstream Elas
poderem acessar o Kafka Então eu vou ter
dado no Delta Para Analytics E eu vou
ter dado para ser consumido por
aplicações E por aí vai dentro do Kafka
Processado também Quando eu estou com
esses três camaradas Kafka Streams By2X
E K5DB Geralmente é quando eu estou
trabalhando somente com Kafka Estou no
topo Unicamente de Kafka Então são
sistemas, aplicações pequenas Mais
simples Continuam com Exato como a
Semantics Fim a fim Onde eu consigo
acessar o Kafka Faço um orçamento
simples E volto para ele Leve, rápido e
ágil Onde eu consigo escalar isso em
microserviços diferentes E por aí vai
Normalmente qual que são as
recomendações Eu uso muito esse cara
aqui agora Por que?
Porque ele virou de fato Uma plataforma
de streaming de processamento no mercado
atual Hoje dificilmente A gente vai
conseguir bater De novo, deployment,
facilidade Gestão, gerenciamento De
jobs, execução dele Tudo Que te entrega
Então preparem -se para ver Muito
conteúdo de Flink com Kafka Já está
acontecendo nos últimos 2 anos Para 3
anos E eu vou tentar fazer o máximo de
conteúdo em português Também para a
gente Então a ideia é a gente também
começar a quebrar Esse silo aqui E
entender mais esse cara No último dia eu
vou comentar uma coisa Que é específica
dele Chama Stream House É uma
arquitetura completamente nova De Stream
House usando Flink São os criadores do
Flink Eu vou falar isso no último dia Na
sexta -feira Perfeito Design Partners
para a gente terminar Já está acabando
Simple Event Processing Eu tenho um
processamento Simples de evento Eu leio
os dados Eu estou mapeando, filtrando
Quero trazer o estado dele E vou trazer
somente o dado filtrado Que a gente já
viu aqui Eu filtrei especificamente um
tópico de Stocks Ou eu quero só venda Ou
então só deletados Pronto, está aqui
Esse a gente considera como Simple Event
Processing Esse é o Partner para esse
tipo de processo E aqui a gente coloca
os logs filtrados Onde tem a Data
Warehouse Ou dentro de um Lake House
Kafka Streams Ou Bytewebs Como vocês
viram os símbolos indicados Na parte de
processamento Esse meio aqui Pode ser o
Kafka
Streams Processamento com estado local
Eu estou processando No mesmo nível Em
que o meu State Store está Fazendo uma
agregação Por exemplo, eu estou com dois
workers Por exemplo, de Flink Em que eu
estou lendo um tópico de Trades E eu
estou agregando esse dado Então eu estou
com duas execuções Dois Tasks rodando
Para processar o dado de Trades Beleza?
Para fazer o processamento do local Eu
tenho um Multiphase Que isso aqui é bem
legal Particularmente falando Que é bem
interessante Eu estou fazendo várias
fases diferentes de processamento Isso
aqui é muito utilizado Beleza?
Onde eu trago o dado da origem Faço um
pré -processamento dele inicial Depois
eu faço outro processamento Então eu não
faço tudo num só Qual que é interessante
disso?
Como eu comentei na questão do reativo A
partir do momento que chegou um dado
novo Esse cara é trigado Que está
olhando para esse tópico Esse cara
também é trigado Que está olhando para
esse tópico E esse cara é gerado Então é
tudo olhando para tópicos Então a partir
do momento que o tópico chegou Ele
começa Computaciona, computaciona,
computaciona Ele só acontece A partir do
momento que esse cara recebe o dado
Então esse é um Multiphase Processing O
nosso Stream Table Onde eu tenho Tanto
um processo de Table Como de Streaming
Onde nesse processo inicial Eu tenho
dois Streams E eu transformo ele num
Table Então eu posso transformar em
Streams em Table E Table em Streams
também Stream Joy Que eu acabei de
mostrar Eu tenho dois Streams Aqui eu
coloco uma Window Uma janela de tempo
Entre esse que eu falo Olha, pega para
mim os dados correspondentes a janela
Então aqui é o ID 47 Que apareceu aqui
dentro E aconteceu no final da janela
Então ele pegou e trouxe o 47 aqui Se
não tivesse correspondente Iria abrir a
janela, executar E não ia fazer nada
Iria seguir para a próxima janela Então
esse aqui é o nosso Stream Joiner
Baseado em janela O Out of Sequence Que
pode acontecer Em que eu tenho dados Em
horários diferentes Pode acontecer O
dado aqui é de 4h45min Ele é dado como
Late Arrival Data Pode acontecer Um
tempo atrás ele chegou E eu processo o
dado No momento que ele chega aqui Na
janela de Late Arrival E eu posso mover
ele Realmente é o que eu gosto de fazer
A gente move ele para tópicos diferentes
E a gente faz um Join posterior na
tabela Então eu consigo resolver dessa
forma Leio Transformo ele em um Tem um
Change Log Faço um reprocessamento Na
tabela Dentro da tabela de jogo final
Adicionando esses novos registros Faço
um reprocessamento Gente, para hoje É só
isso tudo Acabou a
matéria Eu vou abrir para perguntas Quem
quiser perguntar Fica a vontade Como eu
comentei os códigos Posteriormente eu
dou acesso Aqui no repositório Para
vocês acessarem Amanhã a gente fala de
Consumer Então Fizemos tudo aqui Agora a
gente vai colher os louros A gente vai
ler A gente vai também Usar o lápis em
tempo real Então vai ser bem
interessante É uma parte que eu gosto
bastante É a parte final ali Para a
gente ver tudo o que a gente fez
Interligado, um ligando o outro E o dado
processado pronto para ser lido Aqui nas
melhores tecnologias Para a gente poder
encadear É importante que esse
treinamento Ele é sempre focado em Kafka
como centro Mas a gente mostra o
ecossistema como um todo A gente mostra
todo o processo Então é só Kafka É o que
está ao redor dele Beleza?
Alguém quer fazer uma pergunta?
João, pode ir Pode tirar É uma pergunta
Mas não é uma pergunta sobre o conteúdo
de hoje É que por exemplo Tem várias
pessoas que estão aqui Que não são de BH
E o encontro de BH Acho que várias
pessoas aqui Que estão hoje Estão
envolvidas em projetos Que se
beneficiariam de conversas Que vão rolar
no presencial Ela vai ser transmitida?
Vai ter alguma interação com o pessoal
Da...
De fora de BH também?
Eu não tenho certeza Deixa eu fazer o
seguinte João, deixa eu responder agora
Porque se não eu paro a gravação Beleza
Tanto eu pergunto nesses dois da aula Eu
já respondo Não, quanto a aula, agora
não Então segura aí Alguém tem alguma
pergunta da aula?
Só para eu poder parar a gravação Se a
gente quiser eu libero logo todo mundo
Alguém tem?
Levanta Olha, agora o caso para sempre
Estou brincando Pode perguntar amanhã
também Pode comentar a informação e
perguntar amanhã Deixou para a
gravação?
OK A gente vaianya gente vai Mais a
vontade Mais a vontade